{"version":3,"file":"index.mjs","sources":["../incr/binary-classification/lib/model.js","../incr/kmeans/lib/matrix.js","../incr/kmeans/lib/copy_matrix.js","../incr/kmeans/lib/vector.js","../incr/kmeans/lib/copy_vector.js","../incr/kmeans/lib/normalize.js","../incr/kmeans/lib/normalize_matrix.js","../incr/kmeans/lib/standardize.js","../incr/kmeans/lib/standardize_matrix.js","../incr/kmeans/lib/squared_euclidean.js","../incr/kmeans/lib/euclidean.js","../incr/kmeans/lib/dot.js","../incr/kmeans/lib/squared_cosine.js","../incr/kmeans/lib/squared_correlation.js","../incr/kmeans/lib/init_kmeansplusplus.js","../incr/kmeans/lib/find_closest_centroid.js","../incr/kmeans/lib/update_centroid.js","../incr/kmeans/lib/init.js","../incr/kmeans/lib/init_forgy.js","../incr/kmeans/lib/init_sample.js","../incr/kmeans/lib/init_clusters.js","../incr/kmeans/lib/main.js","../incr/sgd-regression/lib/weight_vector.js","../incr/sgd-regression/lib/dot.js","../incr/sgd-regression/lib/regularize.js","../incr/sgd-regression/lib/loss/epsilon_insensitive.js","../incr/sgd-regression/lib/loss/squared_error.js","../incr/sgd-regression/lib/loss/huber.js","../incr/lib/index.js","../incr/binary-classification/lib/main.js","../incr/binary-classification/lib/validate.js","../incr/kmeans/lib/validate.js","../incr/kmeans/lib/stats.js","../incr/kmeans/lib/incrstats.js","../incr/sgd-regression/lib/main.js","../incr/sgd-regression/lib/validate.js","../incr/sgd-regression/lib/eta_factory.js","../lib/index.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable no-restricted-syntax, no-invalid-this */\n\n'use strict';\n\n// MODULES //\n\nimport setReadOnly from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport setReadOnlyAccessor from '@stdlib/utils/define-nonenumerable-read-only-accessor';\nimport format from '@stdlib/string/format';\nimport { ndarray as gdot } from '@stdlib/blas/base/gdot';\nimport { ndarray as gaxpy } from '@stdlib/blas/base/gaxpy';\nimport dcopy from '@stdlib/blas/base/dcopy';\nimport dscal from '@stdlib/blas/base/dscal';\nimport max from '@stdlib/math/base/special/max';\nimport exp from '@stdlib/math/base/special/exp';\nimport pow from '@stdlib/math/base/special/pow';\nimport sigmoid from '@stdlib/math/base/special/expit';\nimport Float64Array from '@stdlib/array/float64';\nimport ndarray from '@stdlib/ndarray/ctor';\nimport shape2strides from '@stdlib/ndarray/base/shape2strides';\nimport numel from '@stdlib/ndarray/base/numel';\nimport vind2bind from '@stdlib/ndarray/base/vind2bind';\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1.0e-7;\nvar MIN_SCALE = 1.0e-11;\nvar LEARNING_RATE_METHODS = {\n\t'basic': '_basicLearningRate',\n\t'constant': '_constantLearningRate',\n\t'invscaling': '_inverseScalingLearningRate',\n\t'pegasos': '_pegasosLearningRate'\n};\nvar LOSS_METHODS = {\n\t'hinge': '_hingeLoss',\n\t'log': '_logLoss',\n\t'modifiedHuber': '_modifiedHuberLoss',\n\t'perceptron': '_perceptronLoss',\n\t'squaredHinge': '_squaredHingeLoss'\n};\n\n\n// MAIN //\n\n/**\n* Model constructor.\n*\n* ## Notes\n*\n* -   The model (weight vector) implementation is inspired by the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @constructor\n* @param {PositiveInteger} N - number of feature weights (excluding bias/intercept term)\n* @param {Options} opts - model options\n* @param {PositiveNumber} opts.lambda - regularization parameter\n* @param {ArrayLikeObject} opts.learningRate - learning rate function and associated parameters\n* @param {string} opts.loss - loss function\n* @param {boolean} opts.intercept - boolean indicating whether to include an intercept\n* @returns {Model} model\n*/\nfunction Model( N, opts ) {\n\tvar len;\n\n\t// Set internal properties:\n\tthis._N = N;\n\tthis._opts = opts;\n\n\tthis._scaleFactor = 1.0;\n\tthis._t = 0; // iteration counter (i.e., number of updates)\n\n\t// Determine the learning rate function:\n\tthis._learningRateMethod = LEARNING_RATE_METHODS[ opts.learningRate[ 0 ] ];\n\n\t// Determine the loss function:\n\tthis._lossMethod = LOSS_METHODS[ opts.loss ];\n\n\t// Determine the number of model coefficients:\n\tlen = N;\n\tif ( opts.intercept ) {\n\t\tlen += 1;\n\t}\n\t// Initialize a model weight vector with all weights set to zero:\n\tthis._weights = new Float64Array( len );\n\n\t// Initialize model coefficients to zero:\n\tthis._coefficients = new ndarray( 'float64', new Float64Array( len ), [ len ], [ 1 ], 0, 'row-major' );\n\n\treturn this;\n}\n\n/**\n* Adds a provided input vector to the model weight vector.\n*\n* @private\n* @name _add\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - input vector\n* @param {number} scale - scale factor\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_add', function add( x, scale ) {\n\tvar s = scale / this._scaleFactor;\n\tvar w = this._weights;\n\n\t// Scale `x` and add to the model weight vector:\n\tgaxpy( x.shape[ 0 ], s, x.data, x.strides[ 0 ], x.offset, w, 1, 0 );\n\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this._opts.intercept ) {\n\t\tw[ this._N ] += s;\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate.\n*\n* ## Notes\n*\n* -   This learning rate function is based on the learning rate function of the same name in the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @name _basicLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_basicLearningRate', function basic() {\n\treturn 10.0 / ( 10.0+this._t );\n});\n\n/**\n* Returns a constant learning rate.\n*\n* @private\n* @name _constantLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_constantLearningRate', function constant() {\n\treturn this._opts.learningRate[ 1 ];\n});\n\n/**\n* Calculates the dot product of the model weight vector and a provided vector `x`.\n*\n* @private\n* @name _dot\n* @memberof Model.prototype\n* @type {Function}\n* @param {NumericArray} buf - ndarray data buffer\n* @param {integer} stride - stride\n* @param {NonNegativeInteger} offset - index offset\n* @returns {number} dot product\n*/\nsetReadOnly( Model.prototype, '_dot', function dot( buf, stride, offset ) {\n\tvar v = gdot( this._N, this._weights, 1, 0, buf, stride, offset );\n\tif ( this._opts.intercept ) {\n\t\tv += this._weights[ this._N ];\n\t}\n\tv *= this._scaleFactor;\n\treturn v;\n});\n\n/**\n* Updates the model weight vector using the hinge loss function.\n*\n* ## Notes\n*\n* -   The hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _hingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_hingeLoss', function hingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) < 1.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate according to an inverse scaling formula.\n*\n* ## Notes\n*\n* -   The inverse scaling formula is defined as\n*\n*     ```tex\n*     \\eta = \\frac{\\eta_0}{t^{k}}\n*     ```\n*\n*     where \\\\(\\eta_0\\\\) is an initial learning rate, \\\\(t\\\\) is the current iteration, and \\\\(k\\\\) is an exponent controlling how quickly the learning rate decreases.\n*\n* @private\n* @name _inverseScalingLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_inverseScalingLearningRate', function invscaling() {\n\tvar params = this._opts.learningRate;\n\treturn params[ 1 ] / pow( this._t, params[ 2 ] );\n});\n\n/**\n* Updates the model weight vector using the log loss function.\n*\n* ## Notes\n*\n* -   The log loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\ln( 1 + \\exp( -y\\,f(x) ) )\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _logLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_logLoss', function logLoss( x, y ) {\n\tvar loss;\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tloss = y / ( 1.0 + exp( y*d ) );\n\tthis._add( x, eta*loss );\n\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the modified Huber loss function.\n*\n* ## Notes\n*\n* -   The modified Huber loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\begin{cases}\n*       \\max(0, 1 - y\\,f(x))^2 & \\textrm{for}\\,\\,y\\,f(x) \\geq -1\\\\\n*       -4y\\,f(x) & \\textrm{otherwise}\n*     \\end{cases}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* ## References\n*\n* -   Zhang, Tong. 2004. \"Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms.\" In _Proceedings of the Twenty-First International Conference on Machine Learning_, 116. New York, NY, USA: Association for Computing Machinery. doi:[10.1145/1015330.1015332][@zhang:2004a].\n*\n* [@zhang:2004a]: https://doi.org/10.1145/1015330.1015332\n*\n* @private\n* @name _modifiedHuberLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_modifiedHuberLoss', function modifiedHuber( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < -1.0 ) {\n\t\tthis._add( x, 4.0*eta*y );\n\t} else {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate using Pegasos.\n*\n* ## References\n*\n* -   Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. \"Pegasos: primal estimated sub-gradient solver for SVM.\" _Mathematical Programming_ 127 (1): 3–30. doi:[10.1007/s10107-010-0420-4][@shalevshwartz:2011a].\n*\n* [@shalevshwartz:2011a]: https://doi.org/10.1007/s10107-010-0420-4\n*\n* @private\n* @name _pegasos\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_pegasosLearningRate', function pegasos() {\n\treturn 1.0 / ( this._opts.lambda*this._t );\n});\n\n/**\n* Updates the model weight vector using the perceptron loss function.\n*\n* ## Notes\n*\n* -   The perceptron loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max(0, -y\\,f(x))\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* -   The perceptron loss function is equivalent to the hinge loss function without a margin.\n*\n* -   The perceptron loss function does not update the model weight vector when the response is correctly classified.\n*\n* ## References\n*\n* -   Rosenblatt, Frank. 1957. \"The Perceptron–a perceiving and recognizing automaton.\" 85-460-1. Buffalo, NY, USA: Cornell Aeronautical Laboratory.\n*\n* @private\n* @name _perceptronLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_perceptronLoss', function perceptron( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) <= 0.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Performs L2 regularization of the model weights.\n*\n* @private\n* @name _regularize\n* @memberof Model.prototype\n* @type {Function}\n* @param {PositiveNumber} eta - learning rate\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_regularize', function regularize( eta ) {\n\tvar lambda = this._opts.lambda;\n\tif ( lambda <= 0.0 ) {\n\t\treturn this;\n\t}\n\tthis._scale( max( 1.0-( eta*lambda ), MIN_SCALING_FACTOR ) );\n\treturn this;\n});\n\n/**\n* Scale the model weight vector by a provided scaling factor.\n*\n* @private\n* @name _scale\n* @memberof Model.prototype\n* @type {Function}\n* @param {number} factor - scaling factor\n* @throws {RangeError} scaling factor must be a positive number\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_scale', function scale( factor ) {\n\tvar s;\n\tif ( factor <= 0.0 ) {\n\t\tthrow new RangeError( format( 'invalid argument. Attempting to scale a weight vector by a nonpositive value. This is likely due to too large a value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n\t// Check whether we need to scale the weight vector to unity in order to avoid numerical issues...\n\ts = this._scaleFactor;\n\tif ( s < MIN_SCALE ) {\n\t\t// Note: we only scale/shrink the feature weights, not the intercept...\n\t\tdscal( this._N, s, this._weights, 1 );\n\t\tthis._scaleFactor = 1.0;\n\t}\n\tthis._scaleFactor *= factor;\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the squared hinge loss function.\n*\n* ## Notes\n*\n* -   The squared hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}^2\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _squaredHingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_squaredHingeLoss', function squaredHingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < 1.0 ) {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Returns the model coefficients.\n*\n* @private\n* @name coefficients\n* @memberof Model.prototype\n* @type {Function}\n* @returns {ndarray} model coefficients\n*/\nsetReadOnlyAccessor( Model.prototype, 'coefficients', function coefficients() {\n\tvar c = this._coefficients.data;\n\tvar w = this._weights;\n\tdcopy( w.length, w, 1, c, 1 );\n\tdscal( this._N, this._scaleFactor, c, 1 );\n\treturn this._coefficients;\n});\n\n/**\n* Returns the number of model features.\n*\n* @private\n* @name nfeatures\n* @memberof Model.prototype\n* @type {PositiveInteger}\n*/\nsetReadOnlyAccessor( Model.prototype, 'nfeatures', function nfeatures() {\n\treturn this._N;\n});\n\n/**\n* Predicts the response value for one or more observation vectors `X`.\n*\n* @private\n* @name predict\n* @memberof Model.prototype\n* @type {Function}\n* @param {ndarray} X - feature vector\n* @param {string} type - prediction type\n* @returns {ndarray} ndarray containing response values\n*/\nsetReadOnly( Model.prototype, 'predict', function predict( X, type ) {\n\tvar ndims;\n\tvar xbuf;\n\tvar ybuf;\n\tvar xsh;\n\tvar ysh;\n\tvar ord;\n\tvar ptr;\n\tvar sxn;\n\tvar sx;\n\tvar sy;\n\tvar ox;\n\tvar M;\n\tvar N;\n\tvar Y;\n\tvar v;\n\tvar i;\n\n\t// Cache input array properties in case of lazy evaluation:\n\txbuf = X.data;\n\txsh = X.shape;\n\tsx = X.strides;\n\tox = X.offset;\n\tord = X.order;\n\n\tndims = xsh.length - 1;\n\n\t// The output array shape is the same as the input array shape without the last dimension (i.e., the number of dimensions is reduced by one)...\n\tysh = [];\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tysh.push( xsh[ i ] );\n\t}\n\t// Create an output array...\n\tif ( ndims === 0 ) {\n\t\tM = 1;\n\t\tybuf = new Float64Array( 1 );\n\t\tsy = [ 0 ];\n\t} else {\n\t\tM = numel( ysh );\n\t\tybuf = new Float64Array( M );\n\t\tsy = shape2strides( ysh, ord );\n\t}\n\tY = new ndarray( 'int8', ybuf, ysh, sy, 0, ord );\n\n\t// Loop over all observation vectors...\n\tN = this._N; // number of features (i.e., size of last `X` dimension)\n\tsxn = sx[ ndims ]; // stride of the last `X` dimension\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Compute the index offset into the underlying data buffer pointing to the start of the current observation vector:\n\t\tptr = vind2bind( xsh, sx, ox, ord, i*N, 'throw' );\n\n\t\t// Compute the dot product of the current observation vector with the model weight vector:\n\t\tv = this._dot( xbuf, sxn, ptr );\n\n\t\t// Determine the output value:\n\t\tif ( type === 'label' ) {\n\t\t\tv = ( v > 0 ) ? 1 : -1;\n\t\t} else if ( type === 'probability' ) {\n\t\t\tv = sigmoid( v );\n\t\t} // else type === 'linear' (i.e., linear predictor)\n\n\t\t// Set the element in the output array:\n\t\tif ( ndims === 0 ) {\n\t\t\tY.iset( v );\n\t\t} else {\n\t\t\tY.iset( i, v );\n\t\t}\n\t}\n\treturn Y;\n});\n\n/**\n* Updates a model given a provided observation vector and response value.\n*\n* @private\n* @name update\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, 'update', function update( x, y ) {\n\tthis._t += 1;\n\treturn this[ this._lossMethod ]( x, y );\n});\n\n\n// EXPORTS //\n\nexport default Model;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport Float64Array from '@stdlib/array/float64';\nimport ctor from '@stdlib/ndarray/ctor';\nimport bctor from '@stdlib/ndarray/base/ctor';\n\n\n// MAIN //\n\n/**\n* Returns a matrix.\n*\n* @private\n* @param {PositiveInteger} m - number of rows\n* @param {PositiveInteger} n - number of columns\n* @param {boolean} bool - boolean indicating whether to create a low-level ndarray\n* @returns {ndarray} matrix\n*/\nfunction createMatrix( m, n, bool ) {\n\tvar strides;\n\tvar buffer;\n\tvar shape;\n\tvar f;\n\n\tif ( bool ) {\n\t\tf = bctor;\n\t} else {\n\t\tf = ctor;\n\t}\n\tbuffer = new Float64Array( m*n );\n\tshape = [ m, n ];\n\tstrides = [ n, 1 ];\n\treturn f( 'float64', buffer, shape, strides, 0, 'row-major' );\n}\n\n\n// EXPORTS //\n\nexport default createMatrix;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { ndarray as gcopy } from '@stdlib/blas/base/gcopy';\n\n\n// MAIN //\n\n/**\n* Copies matrix elements to another matrix.\n*\n* ## Notes\n*\n* -   Why not just use `gcopy` directly? Because `gcopy` 1) assumes only a single stride per strided array and 2) as we cannot assume that a source matrix is single-segment contiguous, we fall back to copying source matrix \"chunks\" (rows) to a destination matrix. Assuming the source matrix is row-major, then the implementation should be reasonably performant.\n*\n* @private\n* @param {ndarray} Y - destination matrix\n* @param {ndarray} X - source matrix\n* @returns {ndarray} destination matrix\n*/\nfunction copyMatrix( Y, X ) { // TODO: once an ndarray engine is written, determine whether this function can be replaced by a standalone package\n\tvar xbuf;\n\tvar ybuf;\n\tvar sx1;\n\tvar sx2;\n\tvar sy1;\n\tvar sy2;\n\tvar ox;\n\tvar oy;\n\tvar M;\n\tvar N;\n\tvar i;\n\n\tM = X.shape[ 0 ];\n\tN = X.shape[ 1 ];\n\n\txbuf = X.data;\n\tybuf = Y.data;\n\n\tsx1 = X.strides[ 0 ];\n\tsx2 = X.strides[ 1 ];\n\n\tsy1 = Y.strides[ 0 ];\n\tsy2 = Y.strides[ 1 ];\n\n\tox = X.offset;\n\toy = Y.offset;\n\n\tfor ( i = 0; i < M; i++ ) {\n\t\tgcopy( N, xbuf, sx2, ox, ybuf, sy2, oy );\n\t\tox += sx1;\n\t\toy += sy1;\n\t}\n\treturn Y;\n}\n\n\n// EXPORTS //\n\nexport default copyMatrix;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport Float64Array from '@stdlib/array/float64';\nimport ctor from '@stdlib/ndarray/ctor';\nimport bctor from '@stdlib/ndarray/base/ctor';\n\n\n// MAIN //\n\n/**\n* Returns a vector.\n*\n* @private\n* @param {PositiveInteger} N - number of elements\n* @param {boolean} bool - boolean indicating whether to create a low-level ndarray\n* @returns {ndarray} vector\n*/\nfunction createVector( N, bool ) {\n\tvar strides;\n\tvar buffer;\n\tvar shape;\n\tvar f;\n\n\tif ( bool ) {\n\t\tf = bctor;\n\t} else {\n\t\tf = ctor;\n\t}\n\tbuffer = new Float64Array( N );\n\tshape = [ N ];\n\tstrides = [ 1 ];\n\treturn f( 'float64', buffer, shape, strides, 0, 'row-major' );\n}\n\n\n// EXPORTS //\n\nexport default createVector;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { ndarray as gcopy } from '@stdlib/blas/base/gcopy';\n\n\n// MAIN //\n\n/**\n* Copies vector elements to another vector.\n*\n* @private\n* @param {ndarray} out - destination vector\n* @param {ndarray} src - source vector\n* @returns {ndarray} destination vector\n*/\nfunction copyVector( out, src ) {\n\tgcopy( src.shape[0], src.data, src.strides[0], src.offset, out.data, out.strides[0], out.offset ); // eslint-disable-line max-len\n\treturn out;\n}\n\n\n// EXPORTS //\n\nexport default copyVector;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport sqrt from '@stdlib/math/base/special/sqrt';\n\n\n// MAIN //\n\n/**\n* Normalizes a vector.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {integer} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @returns {NumericArray} input array\n*/\nfunction normalize( N, X, strideX, offsetX ) { // TODO: eventually remove this function once project has implemented comparable functionality as a standalone package (e.g., BLAS, which may avoid the naive approach susceptible to overflow/overflow due to summing squares and computing the square root)\n\tvar xi;\n\tvar m;\n\tvar v;\n\tvar i;\n\n\tm = 0.0;\n\n\t// Compute the vector magnitude...\n\txi = offsetX;\n\tfor ( i = 0; i < N; i++ ) {\n\t\tv = X[ xi ];\n\t\tm += v * v;\n\t\txi += strideX;\n\t}\n\tm = sqrt( m );\n\n\t// Normalize the vector...\n\txi = offsetX;\n\tfor ( i = 0; i < N; i++ ) {\n\t\tX[ xi ] /= m;\n\t}\n\treturn X;\n}\n\n\n// EXPORTS //\n\nexport default normalize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport norm from './normalize.js';\n\n\n// MAIN //\n\n/**\n* Normalizes matrix elements by row magnitudes.\n*\n* @private\n* @param {ndarray} mat - matrix to normalize\n* @returns {ndarray} input matrix\n*/\nfunction normalize( mat ) { // TODO: eventually remove this function once project has implemented comparable functionality as a standalone package\n\tvar mbuf;\n\tvar sm1;\n\tvar sm2;\n\tvar om;\n\tvar M;\n\tvar N;\n\tvar i;\n\n\tmbuf = mat.data;\n\tM = mat.shape[ 0 ];\n\tN = mat.shape[ 1 ];\n\tsm1 = mat.strides[ 0 ];\n\tsm2 = mat.strides[ 1 ];\n\tom = mat.offset;\n\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Normalize a matrix row:\n\t\tnorm( N, mbuf, sm2, om );\n\n\t\t// Update the index offset to point to the next row:\n\t\tom += sm1;\n\t}\n\treturn mat;\n}\n\n\n// EXPORTS //\n\nexport default normalize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MAIN //\n\n/**\n* Normalizes a vector by standardization.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {integer} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} mean - strided array containing the sample mean along each dimension\n* @param {integer} strideM - stride\n* @param {NonNegativeInteger} offsetM - index offset\n* @param {NumericArray} stdev - strided array containing the standard deviation along each dimension\n* @param {integer} strideS - stride\n* @param {NonNegativeInteger} offsetS - index offset\n* @returns {ndarray} input array\n*/\nfunction standardize( N, X, strideX, offsetX, mean, strideM, offsetM, stdev, strideS, offsetS ) { // eslint-disable-line max-len\n\tvar xi;\n\tvar mi;\n\tvar si;\n\tvar i;\n\n\t// TODO: consider moving to an \"extended\" BLAS package\n\n\txi = offsetX;\n\tmi = offsetM;\n\tsi = offsetS;\n\tfor ( i = 0; i < N; i++ ) {\n\t\tX[ xi ] = ( X[ xi ] - mean[ mi ] ) / stdev[ si ];\n\t\txi += strideX;\n\t\tmi += strideM;\n\t\tsi += strideS;\n\t}\n\treturn X;\n}\n\n\n// EXPORTS //\n\nexport default standardize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport norm from './standardize.js';\n\n\n// MAIN //\n\n/**\n* Normalizes matrix elements by standardization.\n*\n* @private\n* @param {ndarray} mat - matrix to normalize\n* @param {Float64Array} stats - strided array containing the mean and standard deviation along each dimension\n* @returns {ndarray} input matrix\n*/\nfunction standardize( mat, stats ) { // TODO: eventually remove this function once project has implemented comparable functionality as a standalone package\n\tvar mbuf;\n\tvar sm1;\n\tvar sm2;\n\tvar om;\n\tvar M;\n\tvar N;\n\tvar i;\n\n\tmbuf = mat.data;\n\tM = mat.shape[ 0 ];\n\tN = mat.shape[ 1 ];\n\tsm1 = mat.strides[ 0 ];\n\tsm2 = mat.strides[ 1 ];\n\tom = mat.offset;\n\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Normalize a matrix row:\n\t\tnorm( N, mbuf, sm2, om, stats, 2, 0, stats, 2, 1 ); // Magic numbers come from knowing that the `stats` array is interleaved\n\n\t\t// Update the index offset to point to the next row:\n\t\tom += sm1;\n\t}\n\treturn mat;\n}\n\n\n// EXPORTS //\n\nexport default standardize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport euclidean from './euclidean.js';\n\n\n// MAIN //\n\n/**\n* Computes the squared Euclidean distance between two data points.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {PositiveInteger} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} Y - strided array\n* @param {PositiveInteger} strideY - stride\n* @param {NonNegativeInteger} offsetY - index offset\n* @returns {number} squared Euclidean distance\n*/\nfunction squaredEuclidean( N, X, strideX, offsetX, Y, strideY, offsetY ) { // TODO: consider moving to an \"extended\" BLAS package\n\tvar d = euclidean( N, X, strideX, offsetX, Y, strideY, offsetY );\n\treturn d * d;\n}\n\n\n// EXPORTS //\n\nexport default squaredEuclidean;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport sqrt from '@stdlib/math/base/special/sqrt';\n\n\n// MAIN //\n\n/**\n* Computes the Euclidean distance between two vectors.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {PositiveInteger} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} Y - strided array\n* @param {PositiveInteger} strideY - stride\n* @param {NonNegativeInteger} offsetY - index offset\n* @returns {number} Euclidean distance\n*/\nfunction euclidean( N, X, strideX, offsetX, Y, strideY, offsetY ) { // TODO: remove and use BLAS implementation\n\tvar xi;\n\tvar yi;\n\tvar d;\n\tvar s;\n\tvar i;\n\n\txi = offsetX;\n\tyi = offsetY;\n\ts = 0.0;\n\tfor ( i = 0; i < N; i++ ) {\n\t\td = X[ xi ] - Y[ yi ];\n\t\ts += d * d;\n\t\txi += strideX;\n\t\tyi += strideY;\n\t}\n\treturn sqrt( s );\n}\n\n\n// EXPORTS //\n\nexport default euclidean;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MAIN //\n\n/**\n* Computes the dot product of two vectors.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {PositiveInteger} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} Y - strided array\n* @param {PositiveInteger} strideY - stride\n* @param {NonNegativeInteger} offsetY - index offset\n* @returns {number} dot product\n*/\nfunction dot( N, X, strideX, offsetX, Y, strideY, offsetY ) { // TODO: remove and use BLAS implementation\n\tvar xi;\n\tvar yi;\n\tvar s;\n\tvar i;\n\n\txi = offsetX;\n\tyi = offsetY;\n\ts = 0.0;\n\tfor ( i = 0; i < N; i++ ) {\n\t\ts += X[ xi ] * Y[ yi ];\n\t\txi += strideX;\n\t\tyi += strideY;\n\t}\n\treturn s;\n}\n\n\n// EXPORTS //\n\nexport default dot;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport dot from './dot.js';\n\n\n// MAIN //\n\n/**\n* Computes the squared cosine distance between two data points.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {PositiveInteger} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} Y - strided array\n* @param {PositiveInteger} strideY - stride\n* @param {NonNegativeInteger} offsetY - index offset\n* @returns {number} squared cosine distance\n*/\nfunction squaredCosine( N, X, strideX, offsetX, Y, strideY, offsetY ) { // TODO: consider moving to an \"extended\" BLAS package\n\tvar d = 1.0 - dot( N, X, strideX, offsetX, Y, strideY, offsetY );\n\treturn d * d;\n}\n\n\n// EXPORTS //\n\nexport default squaredCosine;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport dot from './dot.js';\n\n\n// MAIN //\n\n/**\n* Computes the squared correlation distance between two data points.\n*\n* @private\n* @param {NonNegativeInteger} N - number of elements\n* @param {NumericArray} X - strided array\n* @param {PositiveInteger} strideX - stride\n* @param {NonNegativeInteger} offsetX - index offset\n* @param {NumericArray} Y - strided array\n* @param {PositiveInteger} strideY - stride\n* @param {NonNegativeInteger} offsetY - index offset\n* @returns {number} squared correlation distance\n*/\nfunction squaredCorrelation( N, X, strideX, offsetX, Y, strideY, offsetY ) { // TODO: consider moving to an \"extended\" BLAS package\n\tvar d = 1.0 - dot( N, X, strideX, offsetX, Y, strideY, offsetY );\n\treturn d * d;\n}\n\n\n// EXPORTS //\n\nexport default squaredCorrelation;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { factory as randint } from '@stdlib/random/base/discrete-uniform';\nimport { factory as randu } from '@stdlib/random/base/mt19937';\nimport PINF from '@stdlib/constants/float64/pinf';\nimport { ndarray as dcopy } from '@stdlib/blas/base/dcopy';\nimport squaredEuclidean from './squared_euclidean.js';\nimport squaredCosine from './squared_cosine.js';\nimport squaredCorrelation from './squared_correlation.js';\n\n\n// FUNCTIONS //\n\n/**\n* Applies a function for calculating the squared distance between each data point and a specified centroid.\n*\n* @private\n* @param {Array} out - output array\n* @param {Function} dist - distance function to apply\n* @param {PositiveInteger} npts - number of data points\n* @param {PositiveInteger} ndims - number of dimensions\n* @param {ndarray} matrix - data point matrix\n* @param {NonNegativeInteger} ci - centroid row index\n* @returns {Array} output array\n*/\nfunction dapply( out, dist, npts, ndims, matrix, ci ) {\n\tvar offsetC;\n\tvar offsetD;\n\tvar strideD;\n\tvar buf;\n\tvar i;\n\n\tbuf = matrix.data;\n\n\tstrideD = matrix.strides[ 0 ];\n\toffsetC = strideD * ci;\n\toffsetD = 0;\n\n\tfor ( i = 0; i < npts; i++ ) {\n\t\tout[ i ] = dist( ndims, buf, 1, offsetD, buf, 1, offsetC ); // Magic number `1` for stride is based on knowing that the matrix is row-major single-segment contiguous\n\t\toffsetD += strideD;\n\t}\n\treturn out;\n}\n\n\n// MAIN //\n\n/**\n* Initializes centroids by performing the k-means++ initialization procedure.\n*\n* ## Method\n*\n* The k-means++ algorithm for choosing initial centroids is as follows:\n*\n* 1.  Select a data point uniformly at random from a data set \\\\( X \\\\). This data point is first centroid and denoted \\\\( c_0 \\\\).\n*\n* 2.  Compute the distance from each data point to \\\\( c_0 \\\\). Denote the distance between \\\\( c_j \\\\) and data point \\\\( m \\\\) as \\\\( d(x_m, c_j) \\\\).\n*\n* 3.  Select the next centroid, \\\\( c_1 \\\\), at random from \\\\( X \\\\) with probability\n*\n*     ```tex\n*     \\frac{d^2(x_m, c_0)}{\\sum_{j=0}^{n-1} d^2(x_j, c_0)}\n*     ```\n*\n*     where \\\\( n \\\\) is the number of data points.\n*\n* 4.  To choose centroid \\\\( j \\\\),\n*\n*     a.   Compute the distances from each data point to each centroid and assign each data point to its closest centroid.\n*\n*     b.   For \\\\( i = 0,\\ldots,n-1 \\\\) and \\\\( p = 0,\\ldots,j-2 \\\\), select centroid \\\\( j \\\\) at random from \\\\( X \\\\) with probability\n*\n*          ```tex\n*          \\frac{d^2(x_i, c_p)}{\\sum_{\\{h; x_h \\exits C_p\\}} d^2(x_h, c_p)}\n*          ```\n*\n*          where \\\\( C_p \\\\) is the set of all data points closest to centroid \\\\( c_p \\\\) and \\\\( x_i \\\\) belongs to \\\\( c_p \\\\).\n*\n*          Stated more plainly, select each subsequent centroid with a probability proportional to the distance from the centroid to the closest centroid already chosen.\n*\n* 5.  Repeat step `4` until \\\\( k \\\\) centroids have been chosen.\n*\n* ## References\n*\n* -   Arthur, David, and Sergei Vassilvitskii. 2007. \"K-means++: The Advantages of Careful Seeding.\" In _Proceedings of the Eighteenth Annual Acm-Siam Symposium on Discrete Algorithms_, 1027–35. SODA '07. Philadelphia, PA, USA: Society for Industrial and Applied Mathematics. <http://dl.acm.org/citation.cfm?id=1283383.1283494>.\n*\n* @private\n* @param {ndarray} out - output centroids `kxd` matrix\n* @param {ndarray} buffer - data buffer\n* @param {string} metric - distance metric\n* @param {PositiveInteger} trials - number of potential centroids per iteration\n* @param {*} seed - PRNG seed\n* @returns {ndarray} centroids\n*/\nfunction kmeansplusplus( out, buffer, metric, trials, seed ) {\n\tvar centroids; // array of indices\n\tvar offsetC;\n\tvar randi;\n\tvar ndims;\n\tvar dhash;\n\tvar probs;\n\tvar rand;\n\tvar npts;\n\tvar csum;\n\tvar bsum;\n\tvar dist;\n\tvar obuf;\n\tvar buf;\n\tvar ind;\n\tvar sb1;\n\tvar sb2;\n\tvar so1;\n\tvar so2;\n\tvar oo;\n\tvar d2;\n\tvar bc;\n\tvar d;\n\tvar c;\n\tvar k;\n\tvar r;\n\tvar i;\n\tvar j;\n\tvar t;\n\n\tk = out.shape[ 0 ];\n\tndims = out.shape[ 1 ];\n\tnpts = buffer.shape[ 0 ];\n\n\tobuf = out.data;\n\tso1 = out.strides[ 0 ];\n\tso2 = out.strides[ 1 ];\n\too = out.offset;\n\n\tbuf = buffer.data;\n\tsb1 = buffer.strides[ 0 ];\n\tsb2 = buffer.strides[ 1 ];\n\n\t// Create seeded PRNGs:\n\trand = randu({\n\t\t'seed': seed\n\t});\n\trandi = randint({\n\t\t'seed': rand()\n\t});\n\trand = rand.normalized;\n\n\t// Determine the distance functions...\n\tif ( metric === 'cosine' ) {\n\t\tdist = squaredCosine;\n\t} else if ( metric === 'correlation' ) {\n\t\tdist = squaredCorrelation;\n\t} else {\n\t\tdist = squaredEuclidean;\n\t}\n\t// 1. Select a data point at random for the first centroid...\n\tc = randi( 0, npts-1 );\n\tif ( k === 1 ) {\n\t\t// For the trivial case of one centroid, we are done which means we can skip to setting the output centroid data...\n\t\treturn dcopy( ndims, buf, sb2, sb1*c, obuf, so2, oo );\n\t}\n\tcentroids = [ c ];\n\n\t// Create a scratch array for storing squared distances:\n\td2 = new Array( ndims );\n\n\t// Create a strided array for storing closest centroid results:\n\tdhash = new Array( npts*2 );\n\tind = 0;\n\tfor ( i = 0; i < npts; i++ ) {\n\t\tdhash[ ind ] = PINF; // squared distance\n\t\tdhash[ ind+1 ] = 0; // index of the closest centroid\n\t\tind += 2; // +stride\n\t}\n\t// Create a scratch array for storing cumulative probabilities:\n\tprobs = new Array( npts );\n\n\t// 2-5. For each data point, compute the distances to each centroid, find the closest centroid, and, based on the distance to the closest centroid, assign a probability to the data point to be chosen as centroid `c_j`...\n\tfor ( j = 1; j < k; j++ ) {\n\t\t// Note: instead of repeatedly computing centroid distances for each data point, we only need to compute the distances for the most recent centroid and to maintain a hash of closest distance results...\n\t\tdapply( d2, dist, npts, ndims, buffer, centroids[ j-1 ] );\n\t\tcsum = 0.0; // total cumulative distance\n\t\tind = 0;\n\t\tfor ( i = 0; i < npts; i++ ) {\n\t\t\tif ( d2[ i ] < dhash[ ind ] ) {\n\t\t\t\tdhash[ ind ] = d2[ i ];\n\t\t\t\tdhash[ ind+1 ] = j - 1;\n\t\t\t\tcsum += d2[ i ];\n\t\t\t} else {\n\t\t\t\tcsum += dhash[ ind ];\n\t\t\t}\n\t\t\tind += 2; // +stride\n\t\t}\n\t\t// Compute the cumulative probabilities...\n\t\tprobs[ 0 ] = dhash[ 0 ] / csum;\n\t\tind = 2;\n\t\tfor ( i = 1; i < npts; i++ ) {\n\t\t\tprobs[ i ] = probs[ i-1 ] + ( dhash[ ind ] / csum );\n\t\t\tind += 2; // +stride\n\t\t}\n\t\t// Based Arthur's and Vassilvitskii's paper \"kmeans++: The Advantages of Careful Seeding\" (see conclusion), randomly select candidate centroids and pick the candidate which minimizes the total squared distance...\n\t\tbsum = PINF; // best sum\n\t\tbc = -1; // best candidate\n\t\tfor ( t = 0; t < trials; t++ ) {\n\t\t\t// Use rejection sampling to handle edge case where the total cumulative probability does not equal unity due to accumulated floating-point errors and is less than `r` (*very* rarely should this require more than one iteration)...\n\t\t\tc = -1;\n\n\t\t\t// Note: the following should never choose an already chosen centroid (why? because a centroid's minimum squared distance is `0`, which means it will either correspond to a cumulative probability of `0` or will correspond to a cumulative probability equal to the previous cumulative probability, thus leading to the equivalent of a no-op iteration)\n\t\t\twhile ( c === -1 ) {\n\t\t\t\tr = rand(); // Note: `r` exists on the interval `[0,1)`\n\t\t\t\tfor ( i = 0; i < npts; i++ ) {\n\t\t\t\t\tif ( r < probs[ i ] ) {\n\t\t\t\t\t\tc = i;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Compute the sum of squared distances were we to include the candidate centroid...\n\t\t\tcsum = 0.0;\n\t\t\toffsetC = sb1 * c;\n\t\t\tind = 0;\n\t\t\tfor ( i = 0; i < npts; i++ ) {\n\t\t\t\td = dist( ndims, buf, 1, sb1*i, buf, 1, offsetC ); // Magic number `1` for stride as matrix is row-major single-segment contiguous\n\t\t\t\tif ( d < dhash[ ind ] ) {\n\t\t\t\t\tcsum += d;\n\t\t\t\t} else {\n\t\t\t\t\tcsum += dhash[ ind ];\n\t\t\t\t}\n\t\t\t\tind += 2; // +stride\n\t\t\t}\n\t\t\t// Determine if the candidate is the best candidate we have seen thus far...\n\t\t\tif ( csum < bsum ) {\n\t\t\t\tbsum = csum;\n\t\t\t\tbc = c;\n\t\t\t}\n\t\t}\n\t\t// Push the \"best\" candidate to our list of centroids:\n\t\tcentroids.push( bc );\n\t}\n\t// 6. Set centroid data...\n\tfor ( i = 0; i < k; i++ ) {\n\t\t// Note: the following is likely to be an \"out-of-order\" copy...\n\t\tdcopy( ndims, buf, sb2, sb1*centroids[i], obuf, so2, oo );\n\t\too += so1;\n\t}\n\treturn out;\n}\n\n\n// EXPORTS //\n\nexport default kmeansplusplus;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport PINF from '@stdlib/constants/float64/pinf';\n\n\n// MAIN //\n\n/**\n* Finds the closest centroid.\n*\n* @private\n* @param {Function} dist - distance function\n* @param {PositiveInteger} k - number of clusters\n* @param {PositiveInteger} ndims - number of dimensions\n* @param {NumericArray} C - strided array containing centroids\n* @param {PositiveInteger} strideC - centroid row stride\n* @param {NonNegativeInteger} offsetC - centroid index offset\n* @param {NumericArray} V - strided array containing a data point\n* @param {integer} strideV - vector stride\n* @param {NonNegativeInteger} offsetV - vector index offset\n* @returns {NonNegativeInteger} centroid index\n*/\nfunction closestCentroid( dist, k, ndims, C, strideC, offsetC, V, strideV, offsetV ) { // eslint-disable-line max-len\n\tvar cd;\n\tvar c;\n\tvar d;\n\tvar i;\n\n\tcd = PINF;\n\tfor ( i = 0; i < k; i++ ) {\n\t\t// Why the magic number `1`? Because we know that the centroids matrix should be row-major single-segment contiguous.\n\t\td = dist( ndims, C, 1, offsetC, V, strideV, offsetV );\n\t\tif ( d < cd ) {\n\t\t\tcd = d;\n\t\t\tc = i;\n\t\t}\n\t\toffsetC += strideC;\n\t}\n\treturn c;\n}\n\n\n// EXPORTS //\n\nexport default closestCentroid;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MAIN //\n\n/**\n* Updates a centroid.\n*\n* ## Notes\n*\n* -   Uses Welford's algorithm for updating an arithmetic mean.\n*\n* @private\n* @param {PositiveInteger} ndims - number of dimensions\n* @param {PositiveInteger} N - number of data points in a cluster\n* @param {NumericArray} C - strided array containing centroids\n* @param {PositiveInteger} strideC - centroid column stride\n* @param {NonNegativeInteger} offsetC - centroid index offset\n* @param {NumericArray} V - strided array containing a data point\n* @param {integer} strideV - vector stride\n* @param {NonNegativeInteger} offsetV - vector index offset\n* @returns {NumericArray} strided array containing centroids\n*/\nfunction updateCentroid( ndims, N, C, strideC, offsetC, V, strideV, offsetV ) {\n\tvar delta;\n\tvar ci;\n\tvar i;\n\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tci = C[ offsetC ];\n\t\tdelta = V[ offsetV ] - ci;\n\t\tci += delta / N;\n\t\tC[ offsetC ] = ci;\n\n\t\toffsetC += strideC;\n\t\toffsetV += strideV;\n\t}\n\treturn C;\n}\n\n\n// EXPORTS //\n\nexport default updateCentroid;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { ndarray as gcopy } from '@stdlib/blas/base/gcopy';\nimport createMatrix from './matrix.js';\nimport norm from './normalize_matrix.js';\nimport standardize from './standardize_matrix.js';\nimport sample from './init_sample.js';\nimport kmeansplusplus from './init_kmeansplusplus.js';\nimport forgy from './init_forgy.js';\nimport clusters from './init_clusters.js';\n\n\n// MAIN //\n\n/**\n* Returns an initialization accumulator for computing initial centroids.\n*\n* @private\n* @param {ndarray} centroids - matrix for storing centroids\n* @param {ndarray} stats - matrix for storing cluster statistics\n* @param {Function} clusterstats - cluster statistics accumulator\n* @param {(Function|void)} incrstats - mean vector accumulator\n* @param {Function} dist - distance function\n* @param {Options} opts - accumulator options\n* @param {string} opts.metric - distance metric\n* @param {Array} opts.init - initialization metric and associated parameters\n* @param {boolean} opts.normalize - boolean indicating whether to normalize incoming data (only relevant for non-Euclidean distance metrics)\n* @param {*} opts.seed - PRNG seed\n* @returns {Function} accumulator\n*/\nfunction init( centroids, stats, clusterstats, incrstats, dist, opts ) {\n\tvar buffer;\n\tvar ndims;\n\n\tndims = centroids.shape[ 1 ];\n\treturn accumulator;\n\n\t/**\n\t* Computes initial centroids and associated cluster statistics.\n\t*\n\t* @private\n\t* @param {ndarray} v - data vector\n\t* @returns {boolean} boolean indicating whether an accumulator has finished computing initial centroids\n\t*/\n\tfunction accumulator( v ) {\n\t\t// If this is the first data vector, we need to begin caching data vectors for future centroid initialization...\n\t\tif ( buffer === void 0 ) {\n\t\t\tbuffer = createMatrix( opts.init[1], ndims, true ); // low-level\n\t\t\tbuffer.count = 0;\n\t\t}\n\t\t// Check if we are still building our cache of data vectors...\n\t\tif ( buffer.count < opts.init[ 1 ] ) {\n\t\t\t// Copy data into the buffer (why? because (1) we have no guarantee that program execution is synchronous, and, thus, we have no guarantee that data vectors will not have been mutated before sampling, and (2) we can freely mutate buffer data, as may be needed during normalization):\n\t\t\tgcopy( ndims, v.data, v.strides[0], v.offset, buffer.data, buffer.strides[1], buffer.strides[0]*buffer.count ); // eslint-disable-line max-len\n\n\t\t\t// Increment the data vector counter:\n\t\t\tbuffer.count += 1;\n\n\t\t\t// Only proceed to perform centroid initialization if the cache is still not full...\n\t\t\tif ( buffer.count < opts.init[ 1 ] ) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\t// If required by the metric, normalize the data vectors along the dimensions...\n\t\tif ( opts.normalize ) {\n\t\t\tif ( opts.metric === 'cosine' ) {\n\t\t\t\tbuffer = norm( buffer );\n\t\t\t} else if ( opts.metric === 'correlation' ) {\n\t\t\t\tbuffer = standardize( buffer, incrstats() );\n\t\t\t}\n\t\t}\n\t\t// Compute initial centroids...\n\t\tif ( opts.init[ 0 ] === 'forgy' ) {\n\t\t\tcentroids = forgy( centroids, buffer, opts.seed );\n\t\t} else if ( opts.init[ 0 ] === 'sample' ) {\n\t\t\tcentroids = sample( centroids, buffer, opts.seed );\n\t\t} else {\n\t\t\tcentroids = kmeansplusplus( centroids, buffer, opts.metric, opts.init[2], opts.seed ); // eslint-disable-line max-len\n\t\t}\n\t\t// Compute initial clusters:\n\t\tclusters( buffer, centroids, stats, clusterstats, dist );\n\n\t\treturn true;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default init;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { factory as factory } from '@stdlib/random/base/discrete-uniform';\nimport incrmean from '@stdlib/stats/incr/mean';\n\n\n// MAIN //\n\n/**\n* Initializes centroids by randomly assigning each data point to cluster and computing centroids.\n*\n* ## References\n*\n* -   Forgy, E. 1965. \"Cluster Analysis of Multivariate Data: Efficiency versus Interpretability of Classification.\" _Biometrics_ 21 (3): 768–69.\n*\n* @private\n* @param {ndarray} out - output centroids `kxd` matrix\n* @param {ndarray} buffer - buffer containing data points\n* @param {*} seed - PRNG seed\n* @returns {ndarray} centroids\n*/\nfunction forgy( out, buffer, seed ) {\n\tvar randi;\n\tvar obuf;\n\tvar npts;\n\tvar buf;\n\tvar sb1;\n\tvar sb2;\n\tvar so1;\n\tvar so2;\n\tvar acc;\n\tvar oo;\n\tvar oa;\n\tvar ob;\n\tvar c;\n\tvar M;\n\tvar N;\n\tvar i;\n\tvar j;\n\n\tM = out.shape[ 0 ];\n\tN = out.shape[ 1 ];\n\n\tobuf = out.data;\n\tso1 = out.strides[ 0 ];\n\tso2 = out.strides[ 1 ];\n\too = out.offset;\n\n\tbuf = buffer.data;\n\tnpts = buffer.shape[ 0 ];\n\tsb1 = buffer.strides[ 0 ];\n\tsb2 = buffer.strides[ 1 ];\n\tob = buffer.offset;\n\n\t// Initialize a PRNG for randomly assigning data points to clusters:\n\trandi = factory( 0, M-1, {\n\t\t'seed': seed\n\t});\n\n\t// Initialize a strided (MxN) array for storing accumulated centroids...\n\tacc = [];\n\tfor ( i = 0; i < M*N; i++ ) {\n\t\tacc.push( incrmean() );\n\t}\n\n\t// Randomly assign each data point to a cluster and update the respective cluster's centroid...\n\tfor ( i = 0; i < npts; i++ ) {\n\t\t// Generate a random cluster index:\n\t\tc = randi();\n\n\t\t// Compute the accumulator index offset:\n\t\toa = N * c;\n\n\t\t// Update the respective cluster centroid:\n\t\tfor ( j = 0; j < N; j++ ) {\n\t\t\tacc[ oa+j ]( buf[ ob+(sb2*j) ] );\n\t\t}\n\t\t// Update the data point index offset:\n\t\tob += sb1;\n\t}\n\t// Update the output matrix...\n\toa = 0;\n\tfor ( i = 0; i < M; i++ ) {\n\t\tfor ( j = 0; j < N; j++ ) {\n\t\t\tobuf[ oo+(so2*j) ] = acc[ oa ]();\n\t\t\toa += 1; // We can simply increment the array pointer as we know that the accumulator array is row-major single-segment contiguous\n\t\t}\n\t\too += so1;\n\t}\n\treturn out;\n}\n\n\n// EXPORTS //\n\nexport default forgy;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { factory as factory } from '@stdlib/random/sample';\nimport { ndarray as dcopy } from '@stdlib/blas/base/dcopy';\n\n\n// MAIN //\n\n/**\n* Initializes centroids by randomly sampling from a data buffer.\n*\n* @private\n* @param {ndarray} out - output centroids `kxd` matrix\n* @param {ndarray} buffer - buffer from which to sample\n* @param {*} seed - PRNG seed\n* @returns {ndarray} centroids\n*/\nfunction sample( out, buffer, seed ) {\n\tvar rand;\n\tvar inds;\n\tvar obuf;\n\tvar buf;\n\tvar sb1;\n\tvar sb2;\n\tvar so2;\n\tvar oo;\n\tvar s;\n\tvar M;\n\tvar N;\n\tvar i;\n\n\tM = out.shape[ 0 ];\n\tN = out.shape[ 1 ];\n\n\tobuf = out.data;\n\tso2 = out.strides[ 1 ];\n\too = out.offset;\n\n\tbuf = buffer.data;\n\tsb1 = buffer.strides[ 0 ];\n\tsb2 = buffer.strides[ 1 ];\n\n\t// Generate an array of data vector indices...\n\tinds = [];\n\tfor ( i = 0; i < buffer.shape[ 0 ]; i++ ) {\n\t\tinds.push( i );\n\t}\n\t// Only randomly sample from the data buffer if the number of centroids is not equal to the number of data vectors...\n\tif ( M === inds.length ) {\n\t\t// Buffer already qualifies as a \"random\" sample:\n\t\ts = inds;\n\t} else {\n\t\t// Create a seeded random sampler (without replacement):\n\t\trand = factory({\n\t\t\t'seed': seed,\n\t\t\t'size': M,\n\t\t\t'mutate': false,\n\t\t\t'replacement': false\n\t\t});\n\n\t\t// Generate a random sample:\n\t\ts = rand( inds );\n\t}\n\t// Update the centroids...\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Note: the following is likely to be an \"out-of-order\" copy...\n\t\tdcopy( N, buf, sb2, sb1*s[i], obuf, so2, oo );\n\t}\n\treturn out;\n}\n\n\n// EXPORTS //\n\nexport default sample;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport closestCentroid from './find_closest_centroid.js';\nimport updateCentroid from './update_centroid.js';\n\n\n// MAIN //\n\n/**\n* Initializes clusters and associated statistics given a set of centroids.\n*\n* ## Notes\n*\n* -   We follow the same approach when calculating cluster statistics as if the centroids had been provided by a user (i.e., not computed internally), as this ensures consistency with how statistics are computed when subsequent data vectors are provided to the accumulator.\n*\n* @private\n* @param {ndarray} data - matrix containing data points\n* @param {ndarray} centroids - matrix containing centroids\n* @param {ndarray} stats - matrix containing cluster statistics\n* @param {Function} acc - cluster statistics accumulator\n* @param {Function} dist - distance function\n*/\nfunction clusters( data, centroids, stats, acc, dist ) {\n\tvar ndims;\n\tvar cbuf;\n\tvar dbuf;\n\tvar npts;\n\tvar sc;\n\tvar sd;\n\tvar oc;\n\tvar od;\n\tvar N;\n\tvar k;\n\tvar c;\n\tvar d;\n\tvar i;\n\n\tk = centroids.shape[ 0 ];\n\tndims = centroids.shape[ 1 ];\n\tnpts = data.shape[ 0 ];\n\n\tcbuf = centroids.data;\n\tsc = centroids.strides[ 0 ];\n\n\tdbuf = data.data;\n\tsd = data.strides[ 0 ];\n\tod = 0;\n\n\tfor ( i = 0; i < npts; i++ ) {\n\t\t// Find the closest centroid by computing the distance from the provided data point to each centroid:\n\t\tc = closestCentroid( dist, k, ndims, cbuf, sc, 0, dbuf, 1, od ); // Magic numbers arise from knowing that matrices are row-major single-segment contiguous\n\n\t\t// Compute the centroids buffer index offset to point to the closest centroid:\n\t\toc = sc * c;\n\n\t\t// Update the closest centroid:\n\t\tN = stats.get( c, 0 ) + 1;\n\t\tupdateCentroid( ndims, N, cbuf, 1, oc, dbuf, 1, od ); // Magic number `1` as we know that these matrices are row-major single-segment contiguous\n\n\t\t// Recompute the distance based on the updated centroid position:\n\t\td = dist( ndims, cbuf, 1, oc, dbuf, 1, od ); // Magic number `1` as we know that these matrices are row-major single-segment contiguous\n\n\t\t// Update cluster statistics:\n\t\tacc( c, d );\n\n\t\t// Increment the data buffer index offset to point to the next data point:\n\t\tod += sd;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default clusters;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isPositiveInteger } from '@stdlib/assert/is-positive-integer';\nimport isMatrixLike from '@stdlib/assert/is-matrix-like';\nimport isVectorLike from '@stdlib/assert/is-vector-like';\nimport setReadOnly from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport format from '@stdlib/string/format';\nimport minstd from '@stdlib/random/base/minstd-shuffle';\nimport floor from '@stdlib/math/base/special/floor';\nimport ln from '@stdlib/math/base/special/ln';\nimport dcopy from '@stdlib/blas/base/dcopy';\nimport createMatrix from './matrix.js';\nimport copyMatrix from './copy_matrix.js';\nimport createVector from './vector.js';\nimport copyVector from './copy_vector.js';\nimport validate from './validate.js';\nimport INIT_DEFAULTS from './init_defaults.json';\nimport initialization from './init.js';\nimport statistics from './stats.js';\nimport incrstatistics from './incrstats.js';\nimport squaredEuclidean from './squared_euclidean.js';\nimport squaredCosine from './squared_cosine.js';\nimport squaredCorrelation from './squared_correlation.js';\nimport closestCentroid from './find_closest_centroid.js';\nimport updateCentroid from './update_centroid.js';\nimport normalize from './normalize.js';\nimport normalizeMatrix from './normalize_matrix.js';\nimport standardize from './standardize.js';\nimport standardizeMatrix from './standardize_matrix.js';\n\n\n// VARIABLES //\n\n// Number of cluster statistics:\nvar NSTATS = 4; // [ n_obs, sum_squared_dist, mean_squared_dist, stdev_squared_dist ]\n\n\n// FUNCTIONS //\n\n/**\n* Returns a results object.\n*\n* @private\n* @param {PositiveInteger} k - number of clusters\n* @param {PositiveInteger} ndims - number of dimensions\n* @returns {Object} results object\n*/\nfunction createResults( k, ndims ) {\n\tvar out = {};\n\tout.centroids = createMatrix( k, ndims, false ); // high-level\n\tout.stats = createMatrix( k, NSTATS, false ); // high-level\n\treturn out;\n}\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally partitions data into `k` clusters.\n*\n* @param {(PositiveInteger|ndarray)} k - number of clusters or a `k x ndims` matrix containing initial centroids\n* @param {PositiveInteger} [ndims] - number of dimensions (should only be provided if provided a numeric `k` argument)\n* @param {Options} [options] - function options\n* @param {string} [options.metric=\"euclidean\"] - distance metric\n* @param {ArrayLikeObject} [options.init] - method for determining initial centroids\n* @param {boolean} [options.normalize=true] - boolean indicating whether to normalize incoming data (only relevant for non-Euclidean distance metrics)\n* @param {boolean} [options.copy=true] - boolean indicating whether to copy incoming data to prevent mutation during normalization\n* @param {*} [options.seed] - PRNG seed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @throws {RangeError} when using sampling to generate initial centroids, the sample size must be greater than or equal to the number of clusters\n* @returns {Function} accumulator function\n*\n* @example\n* import Float64Array from '@stdlib/array/float64';\n* import ndarray from '@stdlib/ndarray/ctor';\n*\n* // Define initial centroid locations:\n* var buffer = [\n*     0.0, 0.0,\n*     1.0, 1.0,\n*     1.0, -1.0,\n*     -1.0, -1.0,\n*     -1.0, 1.0\n* ];\n* var shape = [ 5, 2 ];\n* var strides = [ 2, 1 ];\n* var offset = 0;\n* var order = 'row-major';\n*\n* var centroids = ndarray( 'float64', buffer, shape, strides, offset, order );\n*\n* // Create a k-means accumulator:\n* var accumulator = incrkmeans( centroids );\n*\n* var out = accumulator();\n* // returns {...}\n*\n* // Create a data vector:\n* buffer = new Float64Array( 2 );\n* shape = [ 2 ];\n* strides = [ 1 ];\n*\n* var vec = ndarray( 'float64', buffer, shape, strides, offset, order );\n*\n* // Provide data to the accumulator:\n* vec.set( 0, 2.0 );\n* vec.set( 1, 1.0 );\n*\n* out = accumulator( vec );\n* // returns {...}\n*\n* vec.set( 0, -5.0 );\n* vec.set( 1, 3.14 );\n*\n* out = accumulator( vec );\n* // returns {...}\n*\n* // Retrieve the current cluster results:\n* out = accumulator();\n* // returns {...}\n*/\nfunction incrkmeans() {\n\tvar clusterstats;\n\tvar centroids;\n\tvar incrstats;\n\tvar options;\n\tvar results;\n\tvar vcopy;\n\tvar stats;\n\tvar ndims;\n\tvar dist;\n\tvar opts;\n\tvar init;\n\tvar err;\n\tvar FLG;\n\tvar k;\n\n\tif ( isMatrixLike( arguments[ 0 ] ) ) {\n\t\tk = arguments[ 0 ].shape[ 0 ];\n\t\tndims = arguments[ 0 ].shape[ 1 ];\n\t\tcentroids = createMatrix( k, ndims, true ); // low-level\n\t\tcentroids = copyMatrix( centroids, arguments[ 0 ] );\n\t\tif ( arguments.length > 1 ) {\n\t\t\toptions = arguments[ 1 ];\n\t\t\tFLG = true;\n\t\t}\n\t} else if ( isPositiveInteger( arguments[ 0 ] ) ) {\n\t\tk = arguments[ 0 ];\n\t\tndims = arguments[ 1 ];\n\t\tif ( !isPositiveInteger( ndims ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Argument specifying number of dimensions must be a positive integer. Value: `%s`.', ndims ) );\n\t\t}\n\t\tif ( arguments.length > 2 ) {\n\t\t\toptions = arguments[ 2 ];\n\t\t\tFLG = true;\n\t\t}\n\t} else {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must either be a positive integer specifying the number of clusters or a matrix containing initial centroids. Value: `%s`.', arguments[ 0 ] ) );\n\t}\n\topts = {\n\t\t'metric': 'euclidean',\n\t\t'init': INIT_DEFAULTS[ 'kmeans++' ].slice(),\n\t\t'seed': minstd(),\n\t\t'normalize': true,\n\t\t'copy': true\n\t};\n\topts.init[ 1 ] = k; // Note: this default applies to all initialization methods\n\topts.init[ 2 ] = 2 + floor( ln( k ) ); // Note: from Arthur's and Vassilvitskii's paper \"kmeans++: The Advantages of Careful Seeding\" (see conclusion)\n\tif ( FLG ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tif ( opts.init[ 1 ] < k ) {\n\t\tthrow new RangeError( format( 'invalid option. First `%s` parameter option must be greater than or equal to the number of clusters. Options: `%f`.', 'init', opts.init[ 1 ] ) );\n\t}\n\t// Initialize a results object:\n\tresults = createResults( k, ndims );\n\n\t// Initialize an internal matrix for tabulating cluster statistics:\n\tstats = createMatrix( k, NSTATS, true ); // low-level\n\n\t// Initialize an internal cluster statistics accumulator:\n\tclusterstats = statistics( stats, k );\n\n\t// Initialize metric-related variables...\n\tif ( opts.metric === 'cosine' ) {\n\t\tdist = squaredCosine;\n\n\t\t// Initialize a scratch vector for copying input vectors:\n\t\tif ( opts.copy ) {\n\t\t\tvcopy = createVector( ndims, true ); // low-level\n\t\t}\n\t} else if ( opts.metric === 'correlation' ) {\n\t\tdist = squaredCorrelation;\n\n\t\t// Initialize an accumulator for computing the mean vector and associated standard deviation along each dimension:\n\t\tif ( opts.normalize ) {\n\t\t\tincrstats = incrstatistics( ndims );\n\t\t}\n\t\t// Initialize a scratch vector for copying input vectors:\n\t\tif ( opts.copy ) {\n\t\t\tvcopy = createVector( ndims, true ); // low-level\n\t\t}\n\t} else {\n\t\tdist = squaredEuclidean;\n\t}\n\t// Check if we need to compute initial centroids...\n\tif ( centroids === void 0 ) {\n\t\t// Initialize an internal matrix for storing centroids:\n\t\tcentroids = createMatrix( k, ndims, true ); // low-level\n\n\t\t// Initialize an accumulator for computing initial centroids:\n\t\tinit = initialization( centroids, stats, clusterstats, incrstats, dist, opts ); // eslint-disable-line max-len\n\t} else {\n\t\t// Update cluster results to include the initial centroids (why? so that, even if no data is provided, the `results` object contains the provided centroids):\n\t\tcopyMatrix( results.centroids, centroids );\n\t}\n\t// Attach properties and methods to the accumulator:\n\tsetReadOnly( accumulator, 'seed', opts.seed );\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a data point vector, the accumulator function returns updated cluster results. If not provided a data point vector, the accumulator function returns the current cluster results.\n\t*\n\t* @private\n\t* @param {ndarray} [vec] - data vector\n\t* @throws {TypeError} must provide a 1-dimensional ndarray\n\t* @throws {Error} vector length must match centroid dimensions\n\t* @returns {(Object|null)} cluster results or null\n\t*/\n\tfunction accumulator( vec ) {\n\t\tvar bool;\n\t\tvar cbuf;\n\t\tvar vbuf;\n\t\tvar sbuf;\n\t\tvar sv;\n\t\tvar sc;\n\t\tvar ov;\n\t\tvar oc;\n\t\tvar v;\n\t\tvar N;\n\t\tvar d;\n\t\tvar c;\n\t\tif ( arguments.length === 0 ) {\n\t\t\tif ( init ) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\treturn results;\n\t\t}\n\t\tv = vec; // Why? We mention `arguments` in the function and perform a subsequent reassignment.\n\t\tif ( !isVectorLike( v ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Must provide a one-dimensional ndarray. Value: `%s`.', v ) );\n\t\t}\n\t\tif ( v.shape[ 0 ] !== ndims ) {\n\t\t\tthrow new Error( format( 'invalid argument. Vector length must match centroid dimensions. Expected: `%u``. Actual: `%u``.', ndims, v.shape[ 0 ] ) );\n\t\t}\n\t\t// Check if we need to update the data point mean vector...\n\t\tif ( incrstats ) {\n\t\t\tincrstats( v );\n\t\t}\n\t\t// Check if we have yet to compute initial centroids...\n\t\tif ( init ) {\n\t\t\tbool = init( v );\n\t\t\tif ( bool === false ) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\t// De-reference `init` so that it and its internal variables can be garbage collected:\n\t\t\tinit = void 0;\n\t\t} else {\n\t\t\t// If required by the metric, normalize the data vector...\n\t\t\tif ( opts.normalize ) {\n\t\t\t\tif ( opts.metric === 'cosine' ) {\n\t\t\t\t\tif ( opts.copy ) {\n\t\t\t\t\t\tv = copyVector( vcopy, v );\n\t\t\t\t\t}\n\t\t\t\t\tnormalize( ndims, v.data, v.strides[ 0 ], v.offset );\n\t\t\t\t} else if ( opts.metric === 'correlation' ) {\n\t\t\t\t\tif ( opts.copy ) {\n\t\t\t\t\t\tv = copyVector( vcopy, v );\n\t\t\t\t\t}\n\t\t\t\t\tsbuf = incrstats();\n\n\t\t\t\t\t// Magic numbers come from knowing that `sbuf` is an interleaved strided array...\n\t\t\t\t\tstandardize( ndims, v.data, v.strides[ 0 ], v.offset, sbuf, 2, 0, sbuf, 2, 1 ); // eslint-disable-line max-len\n\t\t\t\t}\n\t\t\t}\n\t\t\tcbuf = centroids.data;\n\t\t\tsc = centroids.strides[ 0 ];\n\n\t\t\tvbuf = v.data;\n\t\t\tsv = v.strides[ 0 ];\n\t\t\tov = v.offset;\n\n\t\t\t// Find the closest centroid by computing the distance from the provided data point to each centroid:\n\t\t\tc = closestCentroid( dist, k, ndims, cbuf, sc, 0, vbuf, sv, ov ); // Magic number `0` for offset as we know that the matrix view begins at the first buffer element\n\n\t\t\t// Compute the centroids buffer index offset to point to the closest centroid:\n\t\t\toc = sc * c;\n\n\t\t\t// Update the closest centroid:\n\t\t\tN = stats.get( c, 0 ) + 1;\n\t\t\tupdateCentroid( ndims, N, cbuf, 1, oc, vbuf, sv, ov ); // Magic number `1` as we know that the matrix is row-major single-segment contiguous\n\n\t\t\t// Recompute the distance based on the updated centroid position:\n\t\t\td = dist( ndims, cbuf, 1, oc, vbuf, sv, ov ); // Magic number `1` as we know that the matrix is row-major single-segment contiguous\n\n\t\t\t// Update cluster statistics:\n\t\t\tclusterstats( c, d );\n\t\t}\n\t\t// Update the results object:\n\t\tdcopy( centroids.length, centroids.data, 1, results.centroids.data, 1 ); // Magic number `1` as we know that these matrices are row-major single-segment contiguous\n\t\tdcopy( stats.length, stats.data, 1, results.stats.data, 1 ); // Magic number `1` as we know that these matrices are row-major single-segment contiguous\n\n\t\treturn results;\n\t}\n\n\t/**\n\t* Computes data point distances to centroids and returns centroid assignment predictions.\n\t*\n\t* @private\n\t* @param {ndarray} [out] - output vector for storing centroid assignment predictions\n\t* @param {ndarray} X - matrix containing data points (`n x d`, where `n` is the number of data points and `d` is the number of dimensions)\n\t* @throws {TypeError} output argument must be a vector\n\t* @throws {TypeError} must provide a matrix\n\t* @throws {Error} vector length must match number of data points\n\t* @throws {Error} number of matrix columns must match centroid dimensions\n\t* @returns {(ndarray|null)} vector containing centroid (index) predictions or null\n\t*/\n\tfunction predict( out, X ) {\n\t\tvar xbuf;\n\t\tvar cbuf;\n\t\tvar npts;\n\t\tvar sx1;\n\t\tvar sx2;\n\t\tvar sc;\n\t\tvar ox;\n\t\tvar x;\n\t\tvar o;\n\t\tvar c;\n\t\tvar i;\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( !isVectorLike( out ) ) {\n\t\t\t\tthrow new TypeError( format( 'invalid argument. Output argument must be a one-dimensional ndarray. Value: `%s`.', out ) );\n\t\t\t}\n\t\t\to = out;\n\t\t\tx = X;\n\t\t} else {\n\t\t\tx = out;\n\t\t}\n\t\tif ( !isMatrixLike( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Must provide a two-dimensional ndarray. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( x.shape[ 1 ] !== ndims ) {\n\t\t\tthrow new Error( format( 'invalid argument. Number of matrix columns must match centroid dimensions. Expected: `%u`. Actual: `%u`.', ndims, x.shape[ 1 ] ) );\n\t\t}\n\t\tif ( o === void 0 ) {\n\t\t\to = createVector( x.shape[ 0 ], false ); // high-level\n\t\t} else if ( o.length !== x.shape[ 0 ] ) {\n\t\t\tthrow new Error( format( 'invalid argument. Output vector length must match the number of data points. Expected: `%u`. Actual: `%u`.', x.shape[ 0 ], o.length ) );\n\t\t}\n\t\tif ( init ) {\n\t\t\treturn null;\n\t\t}\n\t\tnpts = x.shape[ 0 ];\n\n\t\t// If required by the metric, normalize the data vectors along the dimensions...\n\t\tif ( opts.normalize ) {\n\t\t\tif ( opts.metric === 'cosine' ) {\n\t\t\t\tif ( opts.copy ) {\n\t\t\t\t\tx = copyMatrix( createMatrix( npts, ndims, true ), x ); // low-level\n\t\t\t\t}\n\t\t\t\tx = normalizeMatrix( x );\n\t\t\t} else if ( opts.metric === 'correlation' ) {\n\t\t\t\tif ( opts.copy ) {\n\t\t\t\t\tx = copyMatrix( createMatrix( npts, ndims, true ), x ); // low-level\n\t\t\t\t}\n\t\t\t\tx = standardizeMatrix( x, incrstats() );\n\t\t\t}\n\t\t}\n\t\tcbuf = centroids.data;\n\t\tsc = centroids.strides[ 0 ];\n\n\t\txbuf = x.data;\n\t\tsx1 = x.strides[ 0 ];\n\t\tsx2 = x.strides[ 1 ];\n\t\tox = x.offset;\n\n\t\t// For each data point, find the closest centroid...\n\t\tfor ( i = 0; i < npts; i++ ) {\n\t\t\tc = closestCentroid( dist, k, ndims, cbuf, sc, 0, xbuf, sx2, ox ); // Magic number `0` for offset as we know that the matrix view begins at the first buffer element\n\n\t\t\t// Update the output vector:\n\t\t\to.set( i, c );\n\n\t\t\t// Compute the data point buffer index offset to point to the next data point:\n\t\t\tox += sx1;\n\t\t}\n\t\treturn o;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrkmeans;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* The weight vector implementation was inspired by the [sofia-ml]{@link https://code.google.com/archive/p/sofia-ml/} library.\n*/\n\n// MODULES //\n\nimport isPositiveInteger from '@stdlib/assert/is-positive-integer';\nimport isBoolean from '@stdlib/assert/is-boolean';\nimport setReadOnly from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport format from '@stdlib/string/format';\nimport pow from '@stdlib/math/base/special/pow';\nimport dot from './dot.js';\n\n\n// VARIABLES //\n\nvar MIN_SCALE = 1.0e-11;\n\n\n// FUNCTIONS //\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @private\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nfunction scaleTo( factor ) {\n\t/* eslint-disable no-invalid-this */\n\tvar i;\n\tif ( this.scale < MIN_SCALE ) {\n\t\t// Scale vector to one:\n\t\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\t\tthis._data[ i ] *= this.scale;\n\t\t}\n\t\tthis.scale = 1.0;\n\t}\n\n\tthis.norm *= pow( factor, 2 );\n\n\tif ( factor > 0.0 ) {\n\t\tthis.scale *= factor;\n\t} else {\n\t\tthrow new RangeError( format( 'unexpected error. Scaling weight vector by nonpositive value, likely due to too large value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n}\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @private\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nfunction add( x, xScale ) {\n\t/* eslint-disable no-invalid-this */\n\tvar xscaled;\n\tvar inner;\n\tvar i;\n\n\tinner = 0.0;\n\tif ( xScale === void 0 ) {\n\t\txScale = 1.0;\n\t}\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\txscaled = x[ i ] * xScale;\n\t\tinner += this._data[i] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this.intercept ) {\n\t\txscaled = 1.0 * xScale;\n\t\tinner += this._data[ i ] * xscaled;\n\t\tthis._data[ i ] = this._data[ i ] + ( xscaled / this.scale );\n\t}\n\tthis.norm += ( ( dot( x, x ) + ( ( this.intercept ) ? 1.0 : 0.0 ) ) *\n\t\tpow( xScale, 2 ) ) +\n\t\t( 2.0 * this.scale * inner );\n}\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @private\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nfunction innerProduct( x ) {\n\t/* eslint-disable no-invalid-this */\n\tvar ret = 0;\n\tvar i;\n\tfor ( i = 0; i < x.length; i++ ) {\n\t\tret += this._data[ i ] * x[ i ];\n\t}\n\tret += ( this.intercept ) ? this._data[ i ] : 0.0;\n\tret *= this.scale;\n\treturn ret;\n}\n\n\n// MAIN //\n\n/**\n* Creates a WeightVector.\n*\n* @constructor\n* @param {PositiveInteger} dim - number of feature weights (excluding bias/intercept term)\n* @param {boolean} intercept - boolean indicating whether a bias/intercept weight should be implicitly assumed\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} second argument must be a boolean\n*/\nfunction WeightVector( dim, intercept ) {\n\tvar i;\n\tif ( !(this instanceof WeightVector) ) {\n\t\treturn new WeightVector( dim, intercept );\n\t}\n\tif ( !isPositiveInteger( dim ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must be a positive integer. Value: `%s`.', dim ) );\n\t}\n\tif ( !isBoolean( intercept ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a boolean. Value: `%s`.', intercept ) );\n\t}\n\n\tthis.scale = 1.0;\n\tthis.norm = 0.0;\n\tthis.intercept = intercept;\n\tthis.nWeights = dim + ( ( this.intercept ) ? 1 : 0 );\n\n\tthis._data = new Array( this.nWeights );\n\n\t// Initialize weights to zero:\n\tfor ( i = 0; i < this.nWeights; i++ ) {\n\t\tthis._data[ i ] = 0.0;\n\t}\n}\n\n/**\n* Scale elements of the weight vector by the supplied factor.\n*\n* @memberof WeightVector.prototype\n* @function scaleTo\n* @param {number} factor - scaling factor\n* @throws {RangeError} `lambda` times `eta` must be large enough for the scaling weight to be nonnegative\n*/\nsetReadOnly( WeightVector.prototype, 'scaleTo', scaleTo );\n\n/**\n* Adds vector `x` to the weight vector after scaling its elements.\n*\n* @memberof WeightVector.prototype\n* @function add\n* @param {NumericArray} x - vector to add\n* @param {number} [xScale=1.0] - number to scale the elements of x with\n*/\nsetReadOnly( WeightVector.prototype, 'add', add );\n\n/**\n* Calculates the inner product of the weights and supplied vector `x`.\n*\n* @memberof WeightVector.prototype\n* @function innerProduct\n* @param {NumericArray} x - input vector\n* @returns {number} inner product\n*/\nsetReadOnly( WeightVector.prototype, 'innerProduct', innerProduct );\n\n\n// EXPORTS //\n\nexport default WeightVector;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Calculates the dot product of two vectors.\n*\n* @private\n* @param {NumericArray} x - first vector\n* @param {NumericArray} y - second vector\n* @returns {number} dot product\n*\n* @example\n* var x = [ 1.0, 2.0, 3.0 ];\n* var y = [ 1.0, 2.0, 2.0 ];\n*\n* var ret = dot( x, y );\n* // returns 11.0\n*/\nfunction dot( x, y ) {\n\tvar len = x.length;\n\tvar ret = 0;\n\tvar i;\n\n\tfor ( i = 0; i < len; i++ ) {\n\t\tret += x[ i ] * y[ i ];\n\t}\n\treturn ret;\n}\n\n\n// EXPORTS //\n\nexport default dot;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport max from '@stdlib/math/base/special/max';\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1e-7;\n\n\n// MAIN //\n\n/**\n* L2 regularization of feature weights.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} eta - current learning rate\n*/\nfunction regularize( weights, lambda, eta ) {\n\tvar scalingFactor;\n\tif ( lambda > 0.0 ) {\n\t\tscalingFactor = 1.0 - ( eta * lambda );\n\t\tweights.scaleTo( max( scalingFactor, MIN_SCALING_FACTOR ) );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default regularize;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the epsilon-insensitive loss.\n*\n* ## Notes\n*\n* The penalty of the epsilon-insensitive loss is the absolute value of the dot product of the weights and `x` minus `y` whenever the absolute error exceeds epsilon, and zero otherwise.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction epsilonInsensitiveLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default epsilonInsensitiveLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the squared error loss.\n*\n* ## Notes\n*\n* The squared error loss is defined as the squared difference of the observed and fitted value.\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n*/\nfunction squaredErrorLoss( weights, x, y, eta, lambda ) {\n\tvar loss = y - weights.innerProduct( x );\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tweights.add( x, ( eta * loss ) );\n}\n\n\n// EXPORTS //\n\nexport default squaredErrorLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport regularize from './../regularize.js';\n\n\n// MAIN //\n\n/**\n* Given a new observation `(x,y)`, updates the weights using the [Huber loss][1] function.\n*\n* ## Notes\n*\n* The Huber loss uses squared-error loss for observations with error smaller than epsilon in magnitude and linear loss above that in order to decrease the influence of outliers on the model fit.\n*\n* [1]: https://en.wikipedia.org/wiki/Huber_loss\n*\n* @private\n* @param {WeightVector} weights - current model coefficients\n* @param {NumericArray} x - feature vector\n* @param {number} y - response value\n* @param {PositiveNumber} eta - current learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @param {PositiveNumber} epsilon - insensitivity parameter\n*/\nfunction huberLoss( weights, x, y, eta, lambda, epsilon ) {\n\tvar p = weights.innerProduct( x ) - y;\n\n\t// Perform L2 regularization...\n\tregularize( weights, lambda, eta );\n\n\tif ( p > epsilon ) {\n\t\tweights.add( x, -eta );\n\t} else if ( p < -epsilon ) {\n\t\tweights.add( x, +eta );\n\t} else {\n\t\tweights.add( x, -eta * p );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default huberLoss;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2021 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* When adding modules to the namespace, ensure that they are added in alphabetical order according to module name.\n*/\n\n// MODULES //\n\nimport setReadOnly from '@stdlib/utils/define-read-only-property';\n\n\n// MAIN //\n\n/**\n* Top-level namespace.\n*\n* @namespace ns\n*/\nvar ns = {};\n\n/**\n* @name incrBinaryClassification\n* @memberof ns\n* @readonly\n* @type {Function}\n* @see {@link module:@stdlib/ml/incr/binary-classification}\n*/\nimport incrBinaryClassification from './../../incr/binary-classification';\nsetReadOnly( ns, 'incrBinaryClassification', incrBinaryClassification );\n\n/**\n* @name incrkmeans\n* @memberof ns\n* @readonly\n* @type {Function}\n* @see {@link module:@stdlib/ml/incr/kmeans}\n*/\nimport incrkmeans from './../../incr/kmeans';\nsetReadOnly( ns, 'incrkmeans', incrkmeans );\n\n/**\n* @name incrSGDRegression\n* @memberof ns\n* @readonly\n* @type {Function}\n* @see {@link module:@stdlib/ml/incr/sgd-regression}\n*/\nimport incrSGDRegression from './../../incr/sgd-regression';\nsetReadOnly( ns, 'incrSGDRegression', incrSGDRegression );\n\n\n// EXPORTS //\n\nexport default ns;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isPositiveInteger } from '@stdlib/assert/is-positive-integer';\nimport isVectorLike from '@stdlib/assert/is-vector-like';\nimport isndarrayLike from '@stdlib/assert/is-ndarray-like';\nimport setReadOnly from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport format from '@stdlib/string/format';\nimport Model from './model.js';\nimport LEARNING_RATE_DEFAULTS from './learning_rate_defaults.json';\nimport validate from './validate.js';\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally performs binary classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* -   The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of model feature weights using L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {PositiveInteger} N - number of features\n* @param {Options} [options] - options object\n* @param {PositiveNumber} [options.lambda=1.0e-3] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate=['basic']] - learning rate function and associated parameters (one of `basic`, `constant`, or `pegasos`)\n* @param {string} [options.loss='log'] - loss function (one of `hinge`, `log`, `modifiedHuber`, `perceptron`, or `squaredHinge`)\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Function} accumulator\n*\n* @example\n* import Float64Array from '@stdlib/array/float64';\n* import array from '@stdlib/ndarray/array';\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\nfunction incrBinaryClassification( N, options ) {\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\tif ( !isPositiveInteger( N ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must be a positive integer. Value: `%s`.', N ) );\n\t}\n\topts = {\n\t\t'intercept': true,\n\t\t'lambda': 1.0e-4,\n\t\t'learningRate': LEARNING_RATE_DEFAULTS[ 'basic' ].slice(),\n\t\t'loss': 'log'\n\t};\n\tif ( arguments.length > 1 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tmodel = new Model( N, opts );\n\n\t// Attach methods to the accumulator:\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a feature vector and response value, the accumulator function updates a binary classification model; otherwise, the accumulator function returns the current binary classification model coefficients.\n\t*\n\t* @private\n\t* @param {VectorLike} x - feature vector\n\t* @param {integer} y - response value\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray whose length matches the number of model features\n\t* @throws {TypeError} second argument must be either `+1` or `-1`\n\t* @returns {ndarray} one-dimensional ndarray containing model coefficients\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array/float64';\n\t* import array from '@stdlib/ndarray/array';\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Update the model:\n\t* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n\t* var coefs = accumulator( x, 1 );\n\t* // returns <ndarray>\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn model.coefficients;\n\t\t}\n\t\tif ( !isVectorLike( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be either +1 or -1. Value: `%s`.', y ) );\n\t\t}\n\t\tif ( x.shape[ 0 ] !== model.nfeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray of length %u. Actual length: `%u`.', model.nfeatures, x.shape[ 0 ] ) );\n\t\t}\n\t\tmodel.update( x, y );\n\t\treturn model.coefficients;\n\t}\n\n\t/**\n\t* Predicts the response value for one or more observation vectors `X`.\n\t*\n\t* @private\n\t* @param {ndarrayLike} X - ndarray (of size `(...,N)`) containing observation vectors\n\t* @param {string} [type=\"label\"] - prediction type (either `label`, `probability`, or `linear`)\n\t* @throws {TypeError} first argument must be an ndarray\n\t* @throws {TypeError} first argument must be an ndarray whose last dimension matches the number of model features\n\t* @throws {TypeError} second argument must be a recognized/supported prediction \"type\"\n\t* @throws {Error} second argument must be compatible with the model loss function\n\t* @returns {ndarray} ndarray (of size `(...)`) containing response values\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array/float64';\n\t* import array from '@stdlib/ndarray/array';\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Create a new observation vector:\n\t* var x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n\t*\n\t* // Predict the response value:\n\t* var yhat = accumulator.predict( x );\n\t* // returns <ndarray>\n\t*/\n\tfunction predict( X, type ) {\n\t\tvar sh;\n\t\tvar t;\n\t\tif ( !isndarrayLike( X ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray. Value: `%s`.', X ) );\n\t\t}\n\t\tsh = X.shape;\n\t\tif ( sh[ sh.length-1 ] !== N ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray whose last dimension is of size %u. Actual size: `%u`.', N, sh[ sh.length-1 ] ) );\n\t\t}\n\t\tt = 'label';\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( type === 'probability' ) {\n\t\t\t\tif ( opts.loss !== 'log' && opts.loss !== 'modifiedHuber' ) {\n\t\t\t\t\tthrow new Error( format( 'invalid argument. Second argument is incompatible with model loss function. Probability predictions are only supported when the loss function is one of the following: \"%s\". Model loss function: `%s`.', [ 'log', 'modifiedHuber' ].join( '\", \"' ), opts.loss ) );\n\t\t\t\t}\n\t\t\t} else if ( type !== 'label' && type !== 'linear' ) {\n\t\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a string value equal to either \"label\", \"probability\", or \"linear\". Value: `%s`.', type ) );\n\t\t\t}\n\t\t\tt = type;\n\t\t}\n\t\treturn model.predict( X, t );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrBinaryClassification;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isNonNegativeNumber } from '@stdlib/assert/is-nonnegative-number';\nimport { isPrimitive as isPositiveNumber } from '@stdlib/assert/is-positive-number';\nimport { isPrimitive as isNumber } from '@stdlib/assert/is-number';\nimport { isPrimitive as isBoolean } from '@stdlib/assert/is-boolean';\nimport isArrayLikeObject from '@stdlib/assert/is-array-like-object';\nimport isObject from '@stdlib/assert/is-plain-object';\nimport hasOwnProp from '@stdlib/assert/has-own-property';\nimport contains from '@stdlib/assert/contains';\nimport format from '@stdlib/string/format';\nimport LEARNING_RATES from './learning_rates.json';\nimport LOSS_FUNCTIONS from './loss_functions.json';\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate] - learning rate function\n* @param {string} [options.loss] - loss function\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tvar name;\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a nonnegative number. Option: `%s`.', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\tif ( !isArrayLikeObject( options.learningRate ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be an array-like object. Option: `%s`.', 'learningRate', options.learningRate ) );\n\t\t}\n\t\tname = options.learningRate[ 0 ];\n\t\topts.learningRate[ 0 ] = name;\n\t\tif ( !contains( LEARNING_RATES, name ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. First `%s` option must be one of the following: \"%s\". Option: `%s`.', 'learningRate', LEARNING_RATES.join( '\", \"' ), name ) );\n\t\t}\n\t\tif ( options.learningRate.length > 1 ) {\n\t\t\tif ( name === 'constant' || name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 1 ] = options.learningRate[ 1 ];\n\t\t\t\tif ( !isPositiveNumber( opts.learningRate[ 1 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Second `%s` option must be a positive number. Option: `%s`.', 'learningRate', opts.learningRate[ 1 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ( options.learningRate.length > 2 ) {\n\t\t\tif ( name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 2 ] = options.learningRate[ 2 ];\n\t\t\t\tif ( !isNumber( opts.learningRate[ 2 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Third `%s` option must be a number. Option: `%s`.', 'learningRate', opts.learningRate[ 2 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !contains( LOSS_FUNCTIONS, opts.loss ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', LOSS_FUNCTIONS.join( '\", \"' ), opts.loss ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport isObject from '@stdlib/assert/is-plain-object';\nimport hasOwnProp from '@stdlib/assert/has-own-property';\nimport { isPrimitive as isBoolean } from '@stdlib/assert/is-boolean';\nimport isArrayLike from '@stdlib/assert/is-array-like-object';\nimport { isPrimitive as isPositiveInteger } from '@stdlib/assert/is-positive-integer';\nimport contains from '@stdlib/assert/contains';\nimport format from '@stdlib/string/format';\nimport METRICS from './metrics.json';\nimport INIT_METHODS from './init_methods.json';\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {string} [options.metric] - distance metric\n* @param {ArrayLikeObject} [options.init] - method for determining initial centroids\n* @param {boolean} [options.normalize] - boolean indicating whether to normalize incoming data\n* @param {boolean} [options.copy] - boolean indicating whether to copy incoming data to prevent mutation during normalization\n* @param {*} [options.seed] - PRNG seed\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {\n*     'metric': 'euclidean',\n*     'init': [ 'kmeans++', 1, 1 ]\n* };\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'metric' ) ) {\n\t\topts.metric = options.metric;\n\t\tif ( !contains( METRICS, opts.metric ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'metric', METRICS.join( '\", \"' ), opts.metric ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'init' ) ) {\n\t\tif ( !isArrayLike( options.init ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be an array-like object. Option: `%s`.', 'init', options.init ) );\n\t\t}\n\t\tif ( !contains( INIT_METHODS, options.init[ 0 ] ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option method must be one of the following: \"%s\". Option: `%s`.', 'init', INIT_METHODS.join( '\", \"' ), options.init[ 0 ] ) );\n\t\t}\n\t\topts.init[ 0 ] = options.init[ 0 ];\n\t\tif ( options.init.length > 1 ) {\n\t\t\topts.init[ 1 ] = options.init[ 1 ];\n\t\t\tif ( !isPositiveInteger( opts.init[ 1 ] ) ) {\n\t\t\t\treturn new TypeError( format( 'invalid option. First `%s` parameter option must be a positive integer. Option: `%s`.', 'init', opts.init[ 1 ] ) );\n\t\t\t}\n\t\t}\n\t\tif ( options.init.length > 2 ) {\n\t\t\topts.init[ 2 ] = options.init[ 2 ];\n\t\t\tif ( !isPositiveInteger( opts.init[ 2 ] ) ) {\n\t\t\t\treturn new TypeError( format( 'invalid option. Second `%s` parameter option must be a positive integer. Option: `%s`.', 'init', opts.init[ 2 ] ) );\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'normalize' ) ) {\n\t\topts.normalize = options.normalize;\n\t\tif ( !isBoolean( opts.normalize ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'normalize', opts.normalize ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'copy' ) ) {\n\t\topts.copy = options.copy;\n\t\tif ( !isBoolean( opts.copy ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'copy', opts.copy ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'seed' ) ) {\n\t\topts.seed = options.seed;\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport sqrt from '@stdlib/math/base/special/sqrt';\n\n\n// MAIN //\n\n/**\n* Returns an accumulator for computing cluster statistics.\n*\n* @private\n* @param {ndarray} out - matrix for storing cluster statistics\n* @param {PositiveInteger} k - number of clusters\n* @returns {Function} accumulator\n*/\nfunction stats( out, k ) {\n\tvar M2;\n\tvar i;\n\n\t// Create an array for storing second moments:\n\tM2 = [];\n\tfor ( i = 0; i < k; i++ ) {\n\t\tM2.push( 0.0 );\n\t}\n\n\treturn accumulator;\n\n\t/**\n\t* Updates cluster statistics.\n\t*\n\t* @private\n\t* @param {NonNegativeInteger} c - cluster index to which a data point belongs\n\t* @param {number} dist - a data point's squared distance to its respective centroid\n\t* @returns {ndarray} matrix containing cluster statistics\n\t*/\n\tfunction accumulator( c, dist ) {\n\t\tvar delta;\n\t\tvar mu;\n\t\tvar N;\n\n\t\t// Update number of data points belonging to a cluster:\n\t\tN = out.get( c, 0 ) + 1;\n\t\tout.set( c, 0, N );\n\n\t\t// Update the total sum of squared distances within a cluster:\n\t\tout.set( c, 1, out.get( c, 1 )+dist );\n\n\t\t// Update the cluster's squared distance mean and standard deviation (using Welford's algorithm):\n\t\tmu = out.get( c, 2 );\n\t\tdelta = dist - mu;\n\t\tmu += delta / N;\n\t\tM2[ c ] += delta * ( dist-mu );\n\t\tout.set( c, 2, mu );\n\t\tout.set( c, 3, sqrt( M2[c]/(N-1) ) );\n\n\t\treturn out;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default stats;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport incrmeanstdev from '@stdlib/stats/incr/meanstdev';\nimport Float64Array from '@stdlib/array/float64';\n\n\n// MAIN //\n\n/**\n* Initializes incremental accumulators for computing the mean vector and associated standard deviation along each dimension.\n*\n* @private\n* @param {PositiveInteger} ndims - number of dimensions\n* @returns {Object} accumulators\n*/\nfunction incrstats( ndims ) {\n\tvar stride;\n\tvar nstats;\n\tvar acc;\n\tvar out;\n\tvar ob;\n\tvar i;\n\n\t// Define the number of computed statistics:\n\tnstats = 2;\n\n\t// Create a single linear array in which to store accumulated statistics:\n\tout = new Float64Array( ndims*nstats );\n\n\t// Define the array buffer stride (in bytes):\n\tstride = nstats * out.BYTES_PER_ELEMENT;\n\n\t// Initialize accumulators which will write to sections of the linear array:\n\tacc = [];\n\tob = 0;\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tacc.push( incrmeanstdev( new Float64Array( out.buffer, ob, nstats ) ) );\n\t\tob += stride; // buffer offset\n\t}\n\treturn accumulator;\n\n\t/**\n\t* If provided a data point vector, updates the mean vector and associated standard deviation along each dimension. If not provided a data point vector, returns the current mean vector and associated standard deviation along each dimension.\n\t*\n\t* @private\n\t* @param {ndarray} [vec] - data point vector\n\t* @returns {Float64Array} current mean vector and associated standard deviation along each dimension\n\t*/\n\tfunction accumulator( vec ) {\n\t\tvar i;\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn out;\n\t\t}\n\t\tfor ( i = 0; i < ndims; i++ ) {\n\t\t\tacc[ i ]( vec.get( i ) );\n\t\t}\n\t\treturn out;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrstats;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable */ // TODO: fix me\n'use strict';\n\n// MODULES //\n\nimport isArray from '@stdlib/assert/is-array';\nimport format from '@stdlib/string/format';\nimport copy from '@stdlib/utils/copy';\nimport setNonEnumerableReadOnlyAccessor from '@stdlib/utils/define-nonenumerable-read-only-accessor';\nimport setReadOnly from '@stdlib/utils/define-nonenumerable-read-only-property';\nimport WeightVector from './weight_vector.js';\nimport epsilonInsensitiveLoss from './loss/epsilon_insensitive.js';\nimport squaredErrorLoss from './loss/squared_error.js';\nimport huberLoss from './loss/huber.js';\nimport getEta from './eta_factory.js';\nimport DEFAULTS from './defaults.json';\nimport validate from './validate.js';\n\n\n// MAIN //\n\n/**\n* Online learning for regression using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* The sub-gradient of the loss function is estimated for each datum and the regression model is updated incrementally, with a decreasing learning rate and regularization of the feature weights based on L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {Object} [options] - options object\n* @param {PositiveNumber} [options.epsilon=0.1] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0=0.02] - constant learning rate\n* @param {NonNegativeNumber} [options.lambda=1e-3] - regularization parameter\n* @param {string} [options.learningRate='basic'] - string denoting the learning rate to use. Can be `constant`, `pegasos`, or `basic`\n* @param {string} [options.loss='squaredError'] - string denoting the loss function to use. Can be `squaredError`, `epsilonInsensitive`, or `huber`\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Object} regression model\n*\n* @example\n* import incrSGDRegression from '@stdlib/streams/ml/incr/sgd-regression';\n*\n* var accumulator = incrSGDRegression({\n*     'intercept': true\n*     'lambda': 1e-5\n* });\n*\n* // Update model as observations come in:\n* var y = 3.5;\n* var x = [ 2.3, 1.0, 5.0 ];\n* accumulator( x, y );\n*\n* // Predict new observation:\n* var yHat = accumulator.predict( x );\n*\n* // Retrieve coefficients:\n* var coefs = accumulator.coefs;\n*/\nfunction incrSGDRegression( options ) {\n\tvar _nFeatures;\n\tvar _lossfun;\n\tvar _weights;\n\tvar _getEta;\n\tvar accumulator;\n\tvar opts;\n\tvar err;\n\n\topts = copy( DEFAULTS );\n\tif ( arguments.length > 0 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\t_weights = null;\n\n\t// Set loss function:\n\tswitch ( opts.loss ) {\n\tcase 'epsilonInsensitive':\n\t\t_lossfun = epsilonInsensitiveLoss;\n\tbreak;\n\tcase 'huber':\n\t\t_lossfun = huberLoss;\n\tbreak;\n\tcase 'squaredError':\n\t\t_lossfun = squaredErrorLoss;\n\tbreak;\n\tdefault:\n\t\tthrow Error( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', [ 'epsilonInsensitive', 'huber', 'squaredError' ].join( '\", \"' ), opts.loss ) );\n\t}\n\n\t// Set learning rate:\n\t_getEta = getEta( opts.learningRate, opts.eta0, opts.lambda );\n\n\t/**\n\t* Update weights given new observations `y` and `x`.\n\t*\n\t* @param {NumericArray} x - feature vector\n\t* @param {number} y - response value\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t*\n\t* @example\n\t* // Update model as observations come in:\n\t* var y = 3.5;\n\t* var x = [ 2.3, 1.0, 5.0 ];\n\t* accumulator( x, y );\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tvar eta;\n\n\t\tif ( !isArray( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( !_weights ) {\n\t\t\t_weights = new WeightVector( x.length, opts.intercept );\n\t\t\t_nFeatures = opts.intercept ? _weights.nWeights - 1 : _weights.nWeights;\n\t\t}\n\t\tif ( x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures, x ) );\n\t\t}\n\n\t\t// Get current learning rate...\n\t\teta = _getEta();\n\n\t\t// Update weights depending on the chosen loss function...\n\t\t_lossfun( _weights, x, y, eta, opts.lambda, opts.epsilon );\n\t}\n\n\tsetNonEnumerableReadOnlyAccessor( accumulator, 'coefs', getCoefs );\n\tsetReadOnly( accumulator, 'predict', predict );\n\treturn accumulator;\n\n\t/**\n\t* Model coefficients / feature weights.\n\t*\n\t* @private\n\t* @name coefs\n\t* @type {Array}\n\t*\n\t* @example\n\t* // Retrieve coefficients:\n\t* var coefs = accumulator.coefs;\n\t*/\n\tfunction getCoefs() {\n\t\tvar ret;\n\t\tvar i;\n\n\t\tret = new Array( _weights.nWeights );\n\t\tfor ( i = 0; i < ret.length; i++ ) {\n\t\t\tret[ i ] = _weights._data[ i ] * _weights.scale;\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/**\n\t* Predict response for a new observation with features `x`.\n\t*\n\t* @private\n\t* @param {NumericArray} x - feature vector\n\t* @throws {TypeError} first argument must be an array\n\t* @throws {TypeError} first argument must have length equal to the number of predictors\n\t* @returns {number} response value\n\t*\n\t* @example\n\t* // Predict new observation:\n\t* var x = [ 2.3, 5.3, 8.6 ];\n\t* var yHat = accumulator.predict( x );\n\t*/\n\tfunction predict( x ) {\n\t\tif ( !isArray( x ) || x.length !== _nFeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an array of length %u. Value: `%s`.', _nFeatures || 0, x ) );\n\t\t}\n\t\treturn _weights.innerProduct( x );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrSGDRegression;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isNonNegativeNumber } from '@stdlib/assert/is-nonnegative-number';\nimport { isPrimitive as isPositiveNumber } from '@stdlib/assert/is-positive-number';\nimport { isPrimitive as isBoolean } from '@stdlib/assert/is-boolean';\nimport isObject from '@stdlib/assert/is-plain-object';\nimport { isPrimitive as isString } from '@stdlib/assert/is-string';\nimport hasOwnProp from '@stdlib/assert/has-own-property';\nimport format from '@stdlib/string/format';\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.epsilon] - insensitivity parameter\n* @param {PositiveNumber} [options.eta0] - constant learning rate\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {string} [options.learningRate] - the learning rate to use\n* @param {string} [options.loss] -  the loss function to use\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'epsilon' ) ) {\n\t\topts.epsilon = options.epsilon;\n\t\tif ( !isPositiveNumber( opts.epsilon ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a positive number. Option: `%s`.', 'epsilon', opts.epsilon ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'eta0' ) ) {\n\t\topts.eta0 = options.eta0;\n\t\tif ( !isPositiveNumber( opts.eta0 ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a positive number. Option: `%s`.', 'eta0', opts.eta0 ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a nonnegative number. Option: `%s`.', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\topts.learningRate = options.learningRate;\n\t\tif ( !isString( opts.learningRate ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a string. Option: `%s`.', 'learningRate', opts.learningRate ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isString( opts.loss ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a string. Option: `%s`.', 'loss', opts.loss ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport format from '@stdlib/string/format';\n\n\n// MAIN //\n\n/**\n* Returns a function to retrieve the current learning rate.\n*\n* @private\n* @param {string} type - string denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`.\n* @param {PositiveNumber} eta0 - constant learning rate\n* @param {NonNegativeNumber} lambda - regularization parameter\n* @throws {Error} first argument must be `basic`, `constant` or `pegasos`\n* @returns {Function} getEta function\n*/\nfunction closure( type, eta0, lambda ) {\n\tvar iter;\n\tvar ret;\n\n\titer = 1;\n\n\tswitch ( type ) {\n\tcase 'basic':\n\t\t// Default case: 'basic'\n\t\tret = getEtaBasic;\n\t\tbreak;\n\tcase 'constant':\n\t\tret = getEtaConstant;\n\t\tbreak;\n\tcase 'pegasos':\n\t\tret = getEtaPegasos;\n\t\tbreak;\n\tdefault:\n\t\tthrow new Error( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'learningRate', [ 'basic', 'constant', 'pegasos' ].join( '\", \"' ), type ) );\n\t}\n\treturn ret;\n\n\t/**\n\t* Returns the basic learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaBasic() {\n\t\tvar eta = 1000.0 / ( iter + 1000.0 );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n\n\t/**\n\t* Returns the constant learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaConstant() {\n\t\titer += 1;\n\t\treturn eta0;\n\t}\n\n\t/**\n\t* Returns the Pegasos learning rate.\n\t*\n\t* @private\n\t* @returns {number} learning rate\n\t*/\n\tfunction getEtaPegasos() {\n\t\tvar eta = 1.0 / ( lambda * iter );\n\t\titer += 1;\n\t\treturn eta;\n\t}\n}\n\n\n// EXPORTS //\n\nexport default closure;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/*\n* When adding modules to the namespace, ensure that they are added in alphabetical order according to module name.\n*/\n\n// MODULES //\n\nimport setReadOnly from '@stdlib/utils/define-read-only-property';\n\n\n// MAIN //\n\n/**\n* Top-level namespace.\n*\n* @namespace ns\n*/\nvar ns = {};\n\n/**\n* @name incr\n* @memberof ns\n* @readonly\n* @type {Namespace}\n* @see {@link module:@stdlib/ml/incr}\n*/\nimport incr from './../incr';\nsetReadOnly( ns, 'incr', incr );\n\n\n// EXPORTS //\n\nexport default ns;\n"],"names":["LEARNING_RATE_METHODS","basic","constant","invscaling","pegasos","LOSS_METHODS","hinge","log","modifiedHuber","perceptron","squaredHinge","Model","N","opts","len","this","_N","_opts","_scaleFactor","_t","_learningRateMethod","learningRate","_lossMethod","loss","intercept","_weights","Float64Array","_coefficients","ndarray","setReadOnly","prototype","x","scale","s","w","gaxpy","shape","data","strides","offset","buf","stride","v","gdot","y","eta","_regularize","_dot","_add","params","pow","d","exp","lambda","_scale","max","factor","RangeError","format","dscal","setReadOnlyAccessor","c","dcopy","length","X","type","ndims","xbuf","ybuf","xsh","ysh","ord","ptr","sxn","sx","sy","ox","M","Y","i","order","push","numel","shape2strides","vind2bind","sigmoid","iset","createMatrix","m","n","bool","bctor","ctor","copyMatrix","sx1","sx2","sy1","sy2","oy","gcopy","createVector","copyVector","out","src","normalize","strideX","offsetX","xi","sqrt","mat","mbuf","sm1","sm2","om","norm","standardize","mean","strideM","offsetM","stdev","strideS","offsetS","mi","si","stats","squaredEuclidean","strideY","offsetY","yi","euclidean","dot","squaredCosine","squaredCorrelation","dapply","dist","npts","matrix","ci","offsetC","offsetD","strideD","closestCentroid","k","C","strideC","V","strideV","offsetV","cd","PINF","updateCentroid","init","centroids","clusterstats","incrstats","buffer","count","metric","seed","randi","obuf","sb1","sb2","so1","so2","acc","oo","oa","ob","j","factory","incrmean","forgy","inds","size","mutate","replacement","rand","sample","trials","dhash","probs","csum","bsum","ind","d2","bc","r","t","randu","randint","normalized","Array","kmeansplusplus","cbuf","dbuf","sc","sd","oc","od","get","clusters","NSTATS","WeightVector","dim","isPositiveInteger","TypeError","isBoolean","nWeights","_data","xScale","xscaled","inner","ret","MIN_SCALING_FACTOR","regularize","weights","scalingFactor","scaleTo","epsilonInsensitiveLoss","epsilon","p","innerProduct","add","squaredErrorLoss","huberLoss","ns","options","model","err","LEARNING_RATE_DEFAULTS","slice","arguments","name","isObject","hasOwnProp","isNonNegativeNumber","isArrayLikeObject","contains","LEARNING_RATES","join","isPositiveNumber","isNumber","LOSS_FUNCTIONS","validate","accumulator","sh","isndarrayLike","Error","predict","coefficients","isVectorLike","nfeatures","update","results","vcopy","FLG","isMatrixLike","INIT_DEFAULTS","minstd","copy","floor","ln","METRICS","isArrayLike","INIT_METHODS","createResults","M2","delta","mu","set","statistics","BYTES_PER_ELEMENT","incrmeanstdev","vec","incrstatistics","initialization","o","normalizeMatrix","standardizeMatrix","vbuf","sbuf","sv","ov","_nFeatures","_lossfun","_getEta","DEFAULTS","eta0","isString","isArray","iter","getEta","setNonEnumerableReadOnlyAccessor","incr"],"mappings":";;q7JA4CA,IAEIA,GAAwB,CAC3BC,MAAS,qBACTC,SAAY,wBACZC,WAAc,8BACdC,QAAW,wBAERC,GAAe,CAClBC,MAAS,aACTC,IAAO,WACPC,cAAiB,qBACjBC,WAAc,kBACdC,aAAgB,qBAyBjB,SAASC,GAAOC,EAAGC,GAClB,IAAIC,EA0BJ,OAvBAC,KAAKC,GAAKJ,EACVG,KAAKE,MAAQJ,EAEbE,KAAKG,aAAe,EACpBH,KAAKI,GAAK,EAGVJ,KAAKK,oBAAsBpB,GAAuBa,EAAKQ,aAAc,IAGrEN,KAAKO,YAAcjB,GAAcQ,EAAKU,MAGtCT,EAAMF,EACDC,EAAKW,YACTV,GAAO,GAGRC,KAAKU,SAAW,IAAIC,EAAcZ,GAGlCC,KAAKY,cAAgB,IAAIC,EAAS,UAAW,IAAIF,EAAcZ,GAAO,CAAEA,GAAO,CAAE,GAAK,EAAG,aAElFC,IACR,CAaAc,EAAalB,GAAMmB,UAAW,QAAQ,SAAcC,EAAGC,GACtD,IAAIC,EAAID,EAAQjB,KAAKG,aACjBgB,EAAInB,KAAKU,SASb,OANAU,EAAOJ,EAAEK,MAAO,GAAKH,EAAGF,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,OAAQL,EAAG,EAAG,GAG3DnB,KAAKE,MAAMO,YACfU,EAAGnB,KAAKC,KAAQiB,GAEVlB,IACR,IAiBAc,EAAalB,GAAMmB,UAAW,sBAAsB,WACnD,OAAO,IAAS,GAAKf,KAAKI,GAC3B,IAWAU,EAAalB,GAAMmB,UAAW,yBAAyB,WACtD,OAAOf,KAAKE,MAAMI,aAAc,EACjC,IAcAQ,EAAalB,GAAMmB,UAAW,QAAQ,SAAcU,EAAKC,EAAQF,GAChE,IAAIG,EAAIC,EAAM5B,KAAKC,GAAID,KAAKU,SAAU,EAAG,EAAGe,EAAKC,EAAQF,GAKzD,OAJKxB,KAAKE,MAAMO,YACfkB,GAAK3B,KAAKU,SAAUV,KAAKC,KAE1B0B,GAAK3B,KAAKG,YAEX,IA6BAW,EAAalB,GAAMmB,UAAW,cAAc,SAAoBC,EAAGa,GAClE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QAC1B,GACdxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,IACR,IAqBAc,EAAalB,GAAMmB,UAAW,+BAA+B,WAC5D,IAAImB,EAASlC,KAAKE,MAAMI,aACxB,OAAO4B,EAAQ,GAAMC,EAAKnC,KAAKI,GAAI8B,EAAQ,GAC5C,IA6BApB,EAAalB,GAAMmB,UAAW,YAAY,SAAkBC,EAAGa,GAC9D,IAAIrB,EACAsB,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAElBM,EAAIpC,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QACzChB,EAAOqB,GAAM,EAAMQ,EAAKR,EAAEO,IAC1BpC,KAAKiC,KAAMjB,EAAGc,EAAItB,GAEXR,IACR,IAsCAc,EAAalB,GAAMmB,UAAW,sBAAsB,SAAwBC,EAAGa,GAC9E,IAAIC,EACAM,EAWJ,OATAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,UACnC,EACTxB,KAAKiC,KAAMjB,EAAG,EAAIc,EAAID,GAEtB7B,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,IACR,IAiBAc,EAAalB,GAAMmB,UAAW,wBAAwB,WACrD,OAAO,GAAQf,KAAKE,MAAMoC,OAAOtC,KAAKI,GACvC,IAqCAU,EAAalB,GAAMmB,UAAW,mBAAmB,SAAqBC,EAAGa,GACxE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACzB,GACfxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,IACR,IAYAc,EAAalB,GAAMmB,UAAW,eAAe,SAAqBe,GACjE,IAAIQ,EAAStC,KAAKE,MAAMoC,OACxB,OAAKA,GAAU,GAGftC,KAAKuC,OAAQC,EAAK,EAAMV,EAAIQ,EApYJ,OAkYhBtC,IAIT,IAaAc,EAAalB,GAAMmB,UAAW,UAAU,SAAgB0B,GACvD,IAAIvB,EACJ,GAAKuB,GAAU,EACd,MAAM,IAAIC,WAAYC,EAAQ,sJAAuJF,IAUtL,OAPAvB,EAAIlB,KAAKG,cAxZM,QA2ZdyC,EAAO5C,KAAKC,GAAIiB,EAAGlB,KAAKU,SAAU,GAClCV,KAAKG,aAAe,GAErBH,KAAKG,cAAgBsC,EACdzC,IACR,IA6BAc,EAAalB,GAAMmB,UAAW,qBAAqB,SAA2BC,EAAGa,GAChF,IAAIC,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACpC,GACRxB,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,IACR,IAWA6C,EAAqBjD,GAAMmB,UAAW,gBAAgB,WACrD,IAAI+B,EAAI9C,KAAKY,cAAcU,KACvBH,EAAInB,KAAKU,SAGb,OAFAqC,EAAO5B,EAAE6B,OAAQ7B,EAAG,EAAG2B,EAAG,GAC1BF,EAAO5C,KAAKC,GAAID,KAAKG,aAAc2C,EAAG,GAC/B9C,KAAKY,aACb,IAUAiC,EAAqBjD,GAAMmB,UAAW,aAAa,WAClD,OAAOf,KAAKC,EACb,IAaAa,EAAalB,GAAMmB,UAAW,WAAW,SAAkBkC,EAAGC,GAC7D,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAjE,EACAkE,EACApC,EACAqC,EAaJ,IAVAZ,EAAOH,EAAE3B,KACTgC,EAAML,EAAE5B,MACRsC,EAAKV,EAAE1B,QACPsC,EAAKZ,EAAEzB,OACPgC,EAAMP,EAAEgB,MAERd,EAAQG,EAAIN,OAAS,EAGrBO,EAAM,GACAS,EAAI,EAAGA,EAAIb,EAAOa,IACvBT,EAAIW,KAAMZ,EAAKU,IAiBhB,IAde,IAAVb,GACJW,EAAI,EACJT,EAAO,IAAI1C,EAAc,GACzBiD,EAAK,CAAE,KAEPE,EAAIK,EAAOZ,GACXF,EAAO,IAAI1C,EAAcmD,GACzBF,EAAKQ,EAAeb,EAAKC,IAE1BO,EAAI,IAAIlD,EAAS,OAAQwC,EAAME,EAAKK,EAAI,EAAGJ,GAG3C3D,EAAIG,KAAKC,GACTyD,EAAMC,EAAIR,GACJa,EAAI,EAAGA,EAAIF,EAAGE,IAEnBP,EAAMY,EAAWf,EAAKK,EAAIE,EAAIL,EAAKQ,EAAEnE,EAAG,SAGxC8B,EAAI3B,KAAKgC,KAAMoB,EAAMM,EAAKD,GAGZ,UAATP,EACJvB,EAAMA,EAAI,EAAM,GAAK,EACD,gBAATuB,IACXvB,EAAI2C,EAAS3C,IAIC,IAAVwB,EACJY,EAAEQ,KAAM5C,GAERoC,EAAEQ,KAAMP,EAAGrC,GAGb,OAAOoC,CACR,IAaAjD,EAAalB,GAAMmB,UAAW,UAAU,SAAiBC,EAAGa,GAE3D,OADA7B,KAAKI,IAAM,EACJJ,KAAMA,KAAKO,aAAeS,EAAGa,EACrC,0NC/kBA,SAAS2C,GAAcC,EAAGC,EAAGC,GAc5B,OARKA,EACAC,EAEAC,GAKK,UAHD,IAAIlE,EAAc8D,EAAEC,GACrB,CAAED,EAAGC,GACH,CAAEA,EAAG,GAC8B,EAAG,YACjD,CCdA,SAASI,GAAYf,EAAGd,GACvB,IAAIG,EACAC,EACA0B,EACAC,EACAC,EACAC,EACArB,EACAsB,EACArB,EACAjE,EACAmE,EAiBJ,IAfAF,EAAIb,EAAE5B,MAAO,GACbxB,EAAIoD,EAAE5B,MAAO,GAEb+B,EAAOH,EAAE3B,KACT+B,EAAOU,EAAEzC,KAETyD,EAAM9B,EAAE1B,QAAS,GACjByD,EAAM/B,EAAE1B,QAAS,GAEjB0D,EAAMlB,EAAExC,QAAS,GACjB2D,EAAMnB,EAAExC,QAAS,GAEjBsC,EAAKZ,EAAEzB,OACP2D,EAAKpB,EAAEvC,OAEDwC,EAAI,EAAGA,EAAIF,EAAGE,IACnBoB,EAAOvF,EAAGuD,EAAM4B,EAAKnB,EAAIR,EAAM6B,EAAKC,GACpCtB,GAAMkB,EACNI,GAAMF,EAEP,OAAOlB,CACR,CCpCA,SAASsB,GAAcxF,EAAG8E,GAczB,OARKA,EACAC,EAEAC,GAKK,UAHD,IAAIlE,EAAcd,GACnB,CAAEA,GACA,CAAE,GACiC,EAAG,YACjD,CCjBA,SAASyF,GAAYC,EAAKC,GAEzB,OADAJ,EAAOI,EAAInE,MAAM,GAAImE,EAAIlE,KAAMkE,EAAIjE,QAAQ,GAAIiE,EAAIhE,OAAQ+D,EAAIjE,KAAMiE,EAAIhE,QAAQ,GAAIgE,EAAI/D,QAClF+D,CACR,yJCDA,SAASE,GAAW5F,EAAGoD,EAAGyC,EAASC,GAClC,IAAIC,EACAnB,EACA9C,EACAqC,EAMJ,IAJAS,EAAI,EAGJmB,EAAKD,EACC3B,EAAI,EAAGA,EAAInE,EAAGmE,IAEnBS,IADA9C,EAAIsB,EAAG2C,IACEjE,EACTiE,GAAMF,EAMP,IAJAjB,EAAIoB,EAAMpB,GAGVmB,EAAKD,EACC3B,EAAI,EAAGA,EAAInE,EAAGmE,IACnBf,EAAG2C,IAAQnB,EAEZ,OAAOxB,CACR,CC1BA,SAASwC,GAAWK,GACnB,IAAIC,EACAC,EACAC,EACAC,EACApC,EACAjE,EACAmE,EASJ,IAPA+B,EAAOD,EAAIxE,KACXwC,EAAIgC,EAAIzE,MAAO,GACfxB,EAAIiG,EAAIzE,MAAO,GACf2E,EAAMF,EAAIvE,QAAS,GACnB0E,EAAMH,EAAIvE,QAAS,GACnB2E,EAAKJ,EAAItE,OAEHwC,EAAI,EAAGA,EAAIF,EAAGE,IAEnBmC,GAAMtG,EAAGkG,EAAME,EAAKC,GAGpBA,GAAMF,EAEP,OAAOF,CACR,CCpBA,SAASM,GAAavG,EAAGoD,EAAGyC,EAASC,EAASU,EAAMC,EAASC,EAASC,EAAOC,EAASC,GACrF,IAAId,EACAe,EACAC,EACA5C,EAOJ,IAHA4B,EAAKD,EACLgB,EAAKJ,EACLK,EAAKF,EACC1C,EAAI,EAAGA,EAAInE,EAAGmE,IACnBf,EAAG2C,IAAS3C,EAAG2C,GAAOS,EAAMM,IAASH,EAAOI,GAC5ChB,GAAMF,EACNiB,GAAML,EACNM,GAAMH,EAEP,OAAOxD,CACR,CCrBA,SAASmD,GAAaN,EAAKe,GAC1B,IAAId,EACAC,EACAC,EACAC,EACApC,EACAjE,EACAmE,EASJ,IAPA+B,EAAOD,EAAIxE,KACXwC,EAAIgC,EAAIzE,MAAO,GACfxB,EAAIiG,EAAIzE,MAAO,GACf2E,EAAMF,EAAIvE,QAAS,GACnB0E,EAAMH,EAAIvE,QAAS,GACnB2E,EAAKJ,EAAItE,OAEHwC,EAAI,EAAGA,EAAIF,EAAGE,IAEnBmC,GAAMtG,EAAGkG,EAAME,EAAKC,EAAIW,EAAO,EAAG,EAAGA,EAAO,EAAG,GAG/CX,GAAMF,EAEP,OAAOF,CACR,CCnBA,SAASgB,GAAkBjH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GAC9D,IAAI5E,ECDL,SAAoBvC,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GACvD,IAAIpB,EACAqB,EACA7E,EACAlB,EACA8C,EAKJ,IAHA4B,EAAKD,EACLsB,EAAKD,EACL9F,EAAI,EACE8C,EAAI,EAAGA,EAAInE,EAAGmE,IAEnB9C,IADAkB,EAAIa,EAAG2C,GAAO7B,EAAGkD,IACR7E,EACTwD,GAAMF,EACNuB,GAAMF,EAEP,OAAOlB,EAAM3E,EACd,CDhBSgG,CAAWrH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GACvD,OAAO5E,EAAIA,CACZ,CERA,SAAS+E,GAAKtH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GACjD,IAAIpB,EACAqB,EACA/F,EACA8C,EAKJ,IAHA4B,EAAKD,EACLsB,EAAKD,EACL9F,EAAI,EACE8C,EAAI,EAAGA,EAAInE,EAAGmE,IACnB9C,GAAK+B,EAAG2C,GAAO7B,EAAGkD,GAClBrB,GAAMF,EACNuB,GAAMF,EAEP,OAAO7F,CACR,CCVA,SAASkG,GAAevH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GAC3D,IAAI5E,EAAI,EAAM+E,GAAKtH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GACvD,OAAO5E,EAAIA,CACZ,CCHA,SAASiF,GAAoBxH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GAChE,IAAI5E,EAAI,EAAM+E,GAAKtH,EAAGoD,EAAGyC,EAASC,EAAS5B,EAAGgD,EAASC,GACvD,OAAO5E,EAAIA,CACZ,CCEA,SAASkF,GAAQ/B,EAAKgC,EAAMC,EAAMrE,EAAOsE,EAAQC,GAChD,IAAIC,EACAC,EACAC,EACApG,EACAuC,EAQJ,IANAvC,EAAMgG,EAAOnG,KAGbqG,GADAE,EAAUJ,EAAOlG,QAAS,IACNmG,EACpBE,EAAU,EAEJ5D,EAAI,EAAGA,EAAIwD,EAAMxD,IACtBuB,EAAKvB,GAAMuD,EAAMpE,EAAO1B,EAAK,EAAGmG,EAASnG,EAAK,EAAGkG,GACjDC,GAAWC,EAEZ,OAAOtC,CACR,CCrBA,SAASuC,GAAiBP,EAAMQ,EAAG5E,EAAO6E,EAAGC,EAASN,EAASO,EAAGC,EAASC,GAC1E,IAAIC,EACAvF,EACAV,EACA4B,EAGJ,IADAqE,EAAKC,EACCtE,EAAI,EAAGA,EAAI+D,EAAG/D,KAEnB5B,EAAImF,EAAMpE,EAAO6E,EAAG,EAAGL,EAASO,EAAGC,EAASC,IACnCC,IACRA,EAAKjG,EACLU,EAAIkB,GAEL2D,GAAWM,EAEZ,OAAOnF,CACR,CCnBA,SAASyF,GAAgBpF,EAAOtD,EAAGmI,EAAGC,EAASN,EAASO,EAAGC,EAASC,GACnE,IACIV,EACA1D,EAEJ,IAAMA,EAAI,EAAGA,EAAIb,EAAOa,IACvB0D,EAAKM,EAAGL,GAERD,IADQQ,EAAGE,GAAYV,GACT7H,EACdmI,EAAGL,GAAYD,EAEfC,GAAWM,EACXG,GAAWD,EAEZ,OAAOH,CACR,CCLA,SAASQ,GAAMC,EAAW5B,EAAO6B,EAAcC,EAAWpB,EAAMzH,GAC/D,IAAI8I,EACAzF,EAGJ,OADAA,EAAQsF,EAAUpH,MAAO,GAUzB,SAAsBM,QAEL,IAAXiH,KACJA,EAASpE,GAAc1E,EAAK0I,KAAK,GAAIrF,GAAO,IACrC0F,MAAQ,GAGhB,GAAKD,EAAOC,MAAQ/I,EAAK0I,KAAM,KAE9BpD,EAAOjC,EAAOxB,EAAEL,KAAMK,EAAEJ,QAAQ,GAAII,EAAEH,OAAQoH,EAAOtH,KAAMsH,EAAOrH,QAAQ,GAAIqH,EAAOrH,QAAQ,GAAGqH,EAAOC,OAGvGD,EAAOC,OAAS,EAGXD,EAAOC,MAAQ/I,EAAK0I,KAAM,IAC9B,OAAO,EAIJ1I,EAAK2F,YACY,WAAhB3F,EAAKgJ,OACTF,EAASzC,GAAMyC,GACY,gBAAhB9I,EAAKgJ,SAChBF,EAASxC,GAAawC,EAAQD,OAK/BF,EADuB,UAAnB3I,EAAK0I,KAAM,GCnDlB,SAAgBjD,EAAKqD,EAAQG,GAC5B,IAAIC,EACAC,EACAzB,EACA/F,EACAyH,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EAEA3F,EACAjE,EACAmE,EACA0F,EAuBJ,IArBA5F,EAAIyB,EAAIlE,MAAO,GACfxB,EAAI0F,EAAIlE,MAAO,GAEf4H,EAAO1D,EAAIjE,KACX8H,EAAM7D,EAAIhE,QAAS,GACnB8H,EAAM9D,EAAIhE,QAAS,GACnBgI,EAAKhE,EAAI/D,OAETC,EAAMmH,EAAOtH,KACbkG,EAAOoB,EAAOvH,MAAO,GACrB6H,EAAMN,EAAOrH,QAAS,GACtB4H,EAAMP,EAAOrH,QAAS,GACtBkI,EAAKb,EAAOpH,OAGZwH,EAAQW,EAAS,EAAG7F,EAAE,EAAG,CACxBiF,KAAQA,IAITO,EAAM,GACAtF,EAAI,EAAGA,EAAIF,EAAEjE,EAAGmE,IACrBsF,EAAIpF,KAAM0F,KAIX,IAAM5F,EAAI,EAAGA,EAAIwD,EAAMxD,IAAM,CAQ5B,IAHAwF,EAAK3J,EAHDmJ,IAMEU,EAAI,EAAGA,EAAI7J,EAAG6J,IACnBJ,EAAKE,EAAGE,GAAKjI,EAAKgI,EAAIN,EAAIO,IAG3BD,GAAMP,CACN,CAGD,IADAM,EAAK,EACCxF,EAAI,EAAGA,EAAIF,EAAGE,IAAM,CACzB,IAAM0F,EAAI,EAAGA,EAAI7J,EAAG6J,IACnBT,EAAMM,EAAIF,EAAIK,GAAOJ,EAAKE,KAC1BA,GAAM,EAEPD,GAAMH,CACN,CACD,OAAO7D,CACR,CDjBesE,CAAOpB,EAAWG,EAAQ9I,EAAKiJ,MACb,WAAnBjJ,EAAK0I,KAAM,GEzDzB,SAAiBjD,EAAKqD,EAAQG,GAC7B,IACIe,EACAb,EACAxH,EACAyH,EACAC,EACAE,EACAE,EACArI,EACA4C,EACAjE,EACAmE,EAeJ,IAbAF,EAAIyB,EAAIlE,MAAO,GACfxB,EAAI0F,EAAIlE,MAAO,GAEf4H,EAAO1D,EAAIjE,KACX+H,EAAM9D,EAAIhE,QAAS,GACnBgI,EAAKhE,EAAI/D,OAETC,EAAMmH,EAAOtH,KACb4H,EAAMN,EAAOrH,QAAS,GACtB4H,EAAMP,EAAOrH,QAAS,GAGtBuI,EAAO,GACD9F,EAAI,EAAGA,EAAI4E,EAAOvH,MAAO,GAAK2C,IACnC8F,EAAK5F,KAAMF,GAmBZ,IAdC9C,EAFI4C,IAAMgG,EAAK9G,OAEX8G,EAGGH,EAAQ,CACdZ,KAAQA,EACRgB,KAAQjG,EACRkG,QAAU,EACVC,aAAe,GAIZC,CAAMJ,GAGL9F,EAAI,EAAGA,EAAIF,EAAGE,IAEnBjB,EAAOlD,EAAG4B,EAAK0H,EAAKD,EAAIhI,EAAE8C,GAAIiF,EAAMI,EAAKE,GAE1C,OAAOhE,CACR,CFMe4E,CAAQ1B,EAAWG,EAAQ9I,EAAKiJ,MHoB/C,SAAyBxD,EAAKqD,EAAQE,EAAQsB,EAAQrB,GACrD,IAAIN,EACAd,EACAqB,EACA7F,EACAkH,EACAC,EACAJ,EACA1C,EACA+C,EACAC,EACAjD,EACA0B,EACAxH,EACAgJ,EACAvB,EACAC,EACAC,EACAC,EACAE,EACAmB,EACAC,EACAvI,EACAU,EACAiF,EACA6C,EACA5G,EACA0F,EACAmB,EAkCJ,GAhCA9C,EAAIxC,EAAIlE,MAAO,GACf8B,EAAQoC,EAAIlE,MAAO,GACnBmG,EAAOoB,EAAOvH,MAAO,GAErB4H,EAAO1D,EAAIjE,KACX8H,EAAM7D,EAAIhE,QAAS,GACnB8H,EAAM9D,EAAIhE,QAAS,GACnBgI,EAAKhE,EAAI/D,OAETC,EAAMmH,EAAOtH,KACb4H,EAAMN,EAAOrH,QAAS,GACtB4H,EAAMP,EAAOrH,QAAS,GAGtB2I,EAAOY,EAAM,CACZ/B,KAAQA,IAETC,EAAQ+B,EAAQ,CACfhC,KAAQmB,MAETA,EAAOA,EAAKc,WAIXzD,EADe,WAAXuB,EACG1B,GACe,gBAAX0B,EACJzB,GAEAP,GAGRhE,EAAIkG,EAAO,EAAGxB,EAAK,GACR,IAANO,EAEJ,OAAOhF,EAAOI,EAAO1B,EAAK0H,EAAKD,EAAIpG,EAAGmG,EAAMI,EAAKE,GAUlD,IARAd,EAAY,CAAE3F,GAGd4H,EAAK,IAAIO,MAAO9H,GAGhBkH,EAAQ,IAAIY,MAAY,EAALzD,GACnBiD,EAAM,EACAzG,EAAI,EAAGA,EAAIwD,EAAMxD,IACtBqG,EAAOI,GAAQnC,EACf+B,EAAOI,EAAI,GAAM,EACjBA,GAAO,EAMR,IAHAH,EAAQ,IAAIW,MAAOzD,GAGbkC,EAAI,EAAGA,EAAI3B,EAAG2B,IAAM,CAKzB,IAHApC,GAAQoD,EAAInD,EAAMC,EAAMrE,EAAOyF,EAAQH,EAAWiB,EAAE,IACpDa,EAAO,EACPE,EAAM,EACAzG,EAAI,EAAGA,EAAIwD,EAAMxD,IACjB0G,EAAI1G,GAAMqG,EAAOI,IACrBJ,EAAOI,GAAQC,EAAI1G,GACnBqG,EAAOI,EAAI,GAAMf,EAAI,EACrBa,GAAQG,EAAI1G,IAEZuG,GAAQF,EAAOI,GAEhBA,GAAO,EAKR,IAFAH,EAAO,GAAMD,EAAO,GAAME,EAC1BE,EAAM,EACAzG,EAAI,EAAGA,EAAIwD,EAAMxD,IACtBsG,EAAOtG,GAAMsG,EAAOtG,EAAE,GAAQqG,EAAOI,GAAQF,EAC7CE,GAAO,EAKR,IAFAD,EAAOlC,EACPqC,GAAM,EACAE,EAAI,EAAGA,EAAIT,EAAQS,IAAM,CAK9B,IAHA/H,GAAK,GAGU,IAAPA,GAEP,IADA8H,EAAIV,IACElG,EAAI,EAAGA,EAAIwD,EAAMxD,IACtB,GAAK4G,EAAIN,EAAOtG,GAAM,CACrBlB,EAAIkB,EACJ,KACA,CAOH,IAHAuG,EAAO,EACP5C,EAAUuB,EAAMpG,EAChB2H,EAAM,EACAzG,EAAI,EAAGA,EAAIwD,EAAMxD,KACtB5B,EAAImF,EAAMpE,EAAO1B,EAAK,EAAGyH,EAAIlF,EAAGvC,EAAK,EAAGkG,IAC/B0C,EAAOI,GACfF,GAAQnI,EAERmI,GAAQF,EAAOI,GAEhBA,GAAO,EAGHF,EAAOC,IACXA,EAAOD,EACPI,EAAK7H,EAEN,CAED2F,EAAUvE,KAAMyG,EAChB,CAED,IAAM3G,EAAI,EAAGA,EAAI+D,EAAG/D,IAEnBjB,EAAOI,EAAO1B,EAAK0H,EAAKD,EAAIT,EAAUzE,GAAIiF,EAAMI,EAAKE,GACrDA,GAAMH,EAEP,OAAO7D,CACR,CGzKe2F,CAAgBzC,EAAWG,EAAQ9I,EAAKgJ,OAAQhJ,EAAK0I,KAAK,GAAI1I,EAAKiJ,MAKhF,OG5DF,SAAmBzH,EAAMmH,EAAW5B,EAAOyC,EAAK/B,GAC/C,IAAIpE,EACAgI,EACAC,EACA5D,EACA6D,EACAC,EACAC,EACAC,EAEAzD,EACAjF,EAEAkB,EAaJ,IAXA+D,EAAIU,EAAUpH,MAAO,GACrB8B,EAAQsF,EAAUpH,MAAO,GACzBmG,EAAOlG,EAAKD,MAAO,GAEnB8J,EAAO1C,EAAUnH,KACjB+J,EAAK5C,EAAUlH,QAAS,GAExB6J,EAAO9J,EAAKA,KACZgK,EAAKhK,EAAKC,QAAS,GACnBiK,EAAK,EAECxH,EAAI,EAAGA,EAAIwD,EAAMxD,IAKtBuH,EAAKF,GAHLvI,EAAIgF,GAAiBP,EAAMQ,EAAG5E,EAAOgI,EAAME,EAAI,EAAGD,EAAM,EAAGI,IAO3DjD,GAAgBpF,EADZ0D,EAAM4E,IAAK3I,EAAG,GAAM,EACEqI,EAAM,EAAGI,EAAIH,EAAM,EAAGI,GAMhDlC,EAAKxG,EAHDyE,EAAMpE,EAAOgI,EAAM,EAAGI,EAAIH,EAAM,EAAGI,IAMvCA,GAAMF,CAER,CHYEI,CAAU9C,EAAQH,EAAW5B,EAAO6B,EAAcnB,IAE3C,CACP,CACF,CIlDA,IAAIoE,GAAS,EC8Eb,SAASC,GAAcC,EAAKpL,GAC3B,IAAIuD,EACJ,KAAOhE,gBAAgB4L,IACtB,OAAO,IAAIA,GAAcC,EAAKpL,GAE/B,IAAMqL,EAAmBD,GACxB,MAAM,IAAIE,UAAWpJ,EAAQ,4EAA6EkJ,IAE3G,IAAMG,EAAWvL,GAChB,MAAM,IAAIsL,UAAWpJ,EAAQ,oEAAqElC,IAWnG,IARAT,KAAKiB,MAAQ,EACbjB,KAAKmG,KAAO,EACZnG,KAAKS,UAAYA,EACjBT,KAAKiM,SAAWJ,GAAU7L,KAAmB,UAAA,EAAI,GAEjDA,KAAKkM,MAAQ,IAAIjB,MAAOjL,KAAKiM,UAGvBjI,EAAI,EAAGA,EAAIhE,KAAKiM,SAAUjI,IAC/BhE,KAAKkM,MAAOlI,GAAM,CAEpB,CAUAlD,EAAa8K,GAAa7K,UAAW,WArHrC,SAAkB0B,GAEjB,IAAIuB,EACJ,GAAKhE,KAAKiB,MAfK,MAee,CAE7B,IAAM+C,EAAI,EAAGA,EAAIhE,KAAKiM,SAAUjI,IAC/BhE,KAAKkM,MAAOlI,IAAOhE,KAAKiB,MAEzBjB,KAAKiB,MAAQ,CACb,CAID,GAFAjB,KAAKmG,MAAQhE,EAAKM,EAAQ,KAErBA,EAAS,GAGb,MAAM,IAAIC,WAAYC,EAAQ,4HAA6HF,IAF3JzC,KAAKiB,OAASwB,CAIhB,IA6GA3B,EAAa8K,GAAa7K,UAAW,OApGrC,SAAcC,EAAGmL,GAEhB,IAAIC,EACAC,EACArI,EAMJ,IAJAqI,EAAQ,OACQ,IAAXF,IACJA,EAAS,GAEJnI,EAAI,EAAGA,EAAIhD,EAAEgC,OAAQgB,IAC1BoI,EAAUpL,EAAGgD,GAAMmI,EACnBE,GAASrM,KAAKkM,MAAMlI,GAAKoI,EACzBpM,KAAKkM,MAAOlI,GAAMhE,KAAKkM,MAAOlI,GAAQoI,EAAUpM,KAAKiB,MAGjDjB,KAAKS,YACT2L,EAAU,EAAMD,EAChBE,GAASrM,KAAKkM,MAAOlI,GAAMoI,EAC3BpM,KAAKkM,MAAOlI,GAAMhE,KAAKkM,MAAOlI,GAAQoI,EAAUpM,KAAKiB,OAEtDjB,KAAKmG,OC7DN,SAAcnF,EAAGa,GAChB,IAEImC,EAFAjE,EAAMiB,EAAEgC,OACRsJ,EAAM,EAGV,IAAMtI,EAAI,EAAGA,EAAIjE,EAAKiE,IACrBsI,GAAOtL,EAAGgD,GAAMnC,EAAGmC,GAEpB,OAAOsI,CACR,CDoDkBnF,CAAKnG,EAAGA,IAAUhB,KAAmB,UAAA,EAAM,IAC3DmC,EAAKgK,EAAQ,GACX,EAAMnM,KAAKiB,MAAQoL,CACvB,IAsFAvL,EAAa8K,GAAa7K,UAAW,gBA7ErC,SAAuBC,GAEtB,IACIgD,EADAsI,EAAM,EAEV,IAAMtI,EAAI,EAAGA,EAAIhD,EAAEgC,OAAQgB,IAC1BsI,GAAOtM,KAAKkM,MAAOlI,GAAMhD,EAAGgD,GAI7B,OAFAsI,GAAStM,KAAc,UAAKA,KAAKkM,MAAOlI,GAAM,EAC9CsI,GAAOtM,KAAKiB,KAEb,IE3FA,IAAIsL,GAAqB,KAazB,SAASC,GAAYC,EAASnK,EAAQR,GACrC,IAAI4K,EACCpK,EAAS,IACboK,EAAgB,EAAQ5K,EAAMQ,EAC9BmK,EAAQE,QAASnK,EAAKkK,EAAeH,KAEvC,CCJA,SAASK,GAAwBH,EAASzL,EAAGa,EAAGC,EAAKQ,EAAQuK,GAC5D,IAAIC,EAAIL,EAAQM,aAAc/L,GAAMa,EAGpC2K,GAAYC,EAASnK,EAAQR,GAExBgL,EAAID,EACRJ,EAAQO,IAAKhM,GAAIc,GACNgL,GAAKD,GAChBJ,EAAQO,IAAKhM,GAAIc,EAEnB,CCZA,SAASmL,GAAkBR,EAASzL,EAAGa,EAAGC,EAAKQ,GAC9C,IAAI9B,EAAOqB,EAAI4K,EAAQM,aAAc/L,GAGrCwL,GAAYC,EAASnK,EAAQR,GAE7B2K,EAAQO,IAAKhM,EAAKc,EAAMtB,EACzB,CCJA,SAAS0M,GAAWT,EAASzL,EAAGa,EAAGC,EAAKQ,EAAQuK,GAC/C,IAAIC,EAAIL,EAAQM,aAAc/L,GAAMa,EAGpC2K,GAAYC,EAASnK,EAAQR,GAExBgL,EAAID,EACRJ,EAAQO,IAAKhM,GAAIc,GACNgL,GAAKD,EAChBJ,EAAQO,IAAKhM,GAAIc,GAEjB2K,EAAQO,IAAKhM,GAAIc,EAAMgL,EAEzB,gGCrBA,IAAAK,GAAA,CAAA,EAUArM,EAAAqM,GAAA,4BCoCA,SAAmCtN,EAAGuN,GACrC,IAAIC,EACAvN,EACAwN,EAEJ,IAAMxB,EAAmBjM,GACxB,MAAM,IAAIkM,UAAWpJ,EAAQ,4EAA6E9C,IAQ3G,GANAC,EAAO,CACNW,WAAa,EACb6B,OAAU,KACVhC,aAAgBiN,GAAiC,MAACC,QAClDhN,KAAQ,OAEJiN,UAAUzK,OAAS,IACvBsK,ECxCF,SAAmBxN,EAAMsN,GACxB,IAAIM,EACJ,IAAMC,EAAUP,GACf,OAAO,IAAIrB,UAAWpJ,EAAQ,qEAAsEyK,IAErG,GAAKQ,EAAYR,EAAS,eACzBtN,EAAKW,UAAY2M,EAAQ3M,WACnBuL,EAAWlM,EAAKW,YACrB,OAAO,IAAIsL,UAAWpJ,EAAQ,+DAAgE,YAAa7C,EAAKW,YAGlH,GAAKmN,EAAYR,EAAS,YACzBtN,EAAKwC,OAAS8K,EAAQ9K,QAChBuL,EAAqB/N,EAAKwC,SAC/B,OAAO,IAAIyJ,UAAWpJ,EAAQ,0EAA2E,SAAU7C,EAAKwC,SAG1H,GAAKsL,EAAYR,EAAS,gBAAmB,CAC5C,IAAMU,EAAmBV,EAAQ9M,cAChC,OAAO,IAAIyL,UAAWpJ,EAAQ,0EAA2E,eAAgByK,EAAQ9M,eAIlI,GAFAoN,EAAON,EAAQ9M,aAAc,GAC7BR,EAAKQ,aAAc,GAAMoN,GACnBK,EAAUC,GAAgBN,GAC/B,OAAO,IAAI3B,UAAWpJ,EAAQ,sFAAuF,eAAgBqL,GAAeC,KAAM,QAAUP,IAErK,GAAKN,EAAQ9M,aAAa0C,OAAS,IACpB,aAAT0K,GAAgC,eAATA,KAC3B5N,EAAKQ,aAAc,GAAM8M,EAAQ9M,aAAc,IACzC4N,EAAkBpO,EAAKQ,aAAc,KAC1C,OAAO,IAAIyL,UAAWpJ,EAAQ,8EAA+E,eAAgB7C,EAAKQ,aAAc,KAInJ,GAAK8M,EAAQ9M,aAAa0C,OAAS,GACpB,eAAT0K,IACJ5N,EAAKQ,aAAc,GAAM8M,EAAQ9M,aAAc,IACzC6N,EAAUrO,EAAKQ,aAAc,KAClC,OAAO,IAAIyL,UAAWpJ,EAAQ,oEAAqE,eAAgB7C,EAAKQ,aAAc,IAIzI,CACD,OAAKsN,EAAYR,EAAS,UACzBtN,EAAKU,KAAO4M,EAAQ5M,MACduN,EAAUK,GAAgBtO,EAAKU,OAC7B,IAAIuL,UAAWpJ,EAAQ,gFAAiF,OAAQyL,GAAeH,KAAM,QAAUnO,EAAKU,OAGtJ,IACR,CDVQ6N,CAAUvO,EAAMsN,IAErB,MAAME,EAQR,OALAD,EAAQ,IAAIzN,GAAOC,EAAGC,GAGtBgB,EAAawN,EAAa,WA0E1B,SAAkBrL,EAAGC,GACpB,IAAIqL,EACA1D,EACJ,IAAM2D,EAAevL,GACpB,MAAM,IAAI8I,UAAWpJ,EAAQ,oEAAqEM,IAGnG,IADAsL,EAAKtL,EAAE5B,OACEkN,EAAGvL,OAAO,KAAQnD,EAC1B,MAAM,IAAIkM,UAAWpJ,EAAQ,6GAA8G9C,EAAG0O,EAAIA,EAAGvL,OAAO,KAG7J,GADA6H,EAAI,QACC4C,UAAUzK,OAAS,EAAI,CAC3B,GAAc,gBAATE,GACJ,GAAmB,QAAdpD,EAAKU,MAAgC,kBAAdV,EAAKU,KAChC,MAAM,IAAIiO,MAAO9L,EAAQ,0MAA2M,CAAE,MAAO,iBAAkBsL,KAAM,QAAUnO,EAAKU,YAE/Q,GAAc,UAAT0C,GAA6B,WAATA,EAC/B,MAAM,IAAI6I,UAAWpJ,EAAQ,6HAA8HO,IAE5J2H,EAAI3H,CACJ,CACD,OAAOmK,EAAMqB,QAASzL,EAAG4H,EACzB,IA9FMyD,EA2BP,SAASA,EAAatN,EAAGa,GACxB,GAA0B,IAArB4L,UAAUzK,OACd,OAAOqK,EAAMsB,aAEd,IAAMC,EAAc5N,GACnB,MAAM,IAAI+K,UAAWpJ,EAAQ,mFAAoF3B,IAElH,IAAY,IAAPa,GAAkB,IAANA,EAChB,MAAM,IAAIkK,UAAWpJ,EAAQ,0EAA2Ed,IAEzG,GAAKb,EAAEK,MAAO,KAAQgM,EAAMwB,UAC3B,MAAM,IAAI9C,UAAWpJ,EAAQ,wGAAyG0K,EAAMwB,UAAW7N,EAAEK,MAAO,KAGjK,OADAgM,EAAMyB,OAAQ9N,EAAGa,GACVwL,EAAMsB,YACb,CAqDF,IDlJA7N,EAAAqM,GAAA,cPwFA,WACC,IAAIzE,EACAD,EACAE,EACAyE,EACA2B,EACAC,EACAnI,EACA1D,EACAoE,EACAzH,EACA0I,EACA8E,EACA2B,EACAlH,EAEJ,GAAKmH,EAAczB,UAAW,IAC7B1F,EAAI0F,UAAW,GAAIpM,MAAO,GAC1B8B,EAAQsK,UAAW,GAAIpM,MAAO,GAE9BoH,EAAY3D,GADZ2D,EAAYjE,GAAcuD,EAAG5E,GAAO,GACDsK,UAAW,IACzCA,UAAUzK,OAAS,IACvBoK,EAAUK,UAAW,GACrBwB,GAAM,OAED,KAAKnD,EAAmB2B,UAAW,IAWzC,MAAM,IAAI1B,UAAWpJ,EAAQ,8JAA+J8K,UAAW,KARvM,GAFA1F,EAAI0F,UAAW,IAET3B,EADN3I,EAAQsK,UAAW,IAElB,MAAM,IAAI1B,UAAWpJ,EAAQ,sGAAuGQ,IAEhIsK,UAAUzK,OAAS,IACvBoK,EAAUK,UAAW,GACrBwB,GAAM,EAIP,CAUD,IATAnP,EAAO,CACNgJ,OAAU,YACVN,KAAQ2G,GAAe,YAAa3B,QACpCzE,KAAQqG,IACR3J,WAAa,EACb4J,MAAQ,IAEJ7G,KAAM,GAAMT,EACjBjI,EAAK0I,KAAM,GAAM,EAAI8G,EAAOC,EAAIxH,IAC3BkH,IACJ3B,EUrIF,SAAmBxN,EAAMsN,GACxB,IAAMO,EAAUP,GACf,OAAO,IAAIrB,UAAWpJ,EAAQ,qEAAsEyK,IAErG,GAAKQ,EAAYR,EAAS,YACzBtN,EAAKgJ,OAASsE,EAAQtE,QAChBiF,EAAUyB,GAAS1P,EAAKgJ,SAC7B,OAAO,IAAIiD,UAAWpJ,EAAQ,gFAAiF,SAAU6M,GAAQvB,KAAM,QAAUnO,EAAKgJ,SAGxJ,GAAK8E,EAAYR,EAAS,QAAW,CACpC,IAAMqC,EAAarC,EAAQ5E,MAC1B,OAAO,IAAIuD,UAAWpJ,EAAQ,0EAA2E,OAAQyK,EAAQ5E,OAE1H,IAAMuF,EAAU2B,GAActC,EAAQ5E,KAAM,IAC3C,OAAO,IAAIuD,UAAWpJ,EAAQ,uFAAwF,OAAQ+M,GAAazB,KAAM,QAAUb,EAAQ5E,KAAM,KAG1K,GADA1I,EAAK0I,KAAM,GAAM4E,EAAQ5E,KAAM,GAC1B4E,EAAQ5E,KAAKxF,OAAS,IAC1BlD,EAAK0I,KAAM,GAAM4E,EAAQ5E,KAAM,IACzBsD,EAAmBhM,EAAK0I,KAAM,KACnC,OAAO,IAAIuD,UAAWpJ,EAAQ,wFAAyF,OAAQ7C,EAAK0I,KAAM,KAG5I,GAAK4E,EAAQ5E,KAAKxF,OAAS,IAC1BlD,EAAK0I,KAAM,GAAM4E,EAAQ5E,KAAM,IACzBsD,EAAmBhM,EAAK0I,KAAM,KACnC,OAAO,IAAIuD,UAAWpJ,EAAQ,yFAA0F,OAAQ7C,EAAK0I,KAAM,IAG7I,CACD,OAAKoF,EAAYR,EAAS,eACzBtN,EAAK2F,UAAY2H,EAAQ3H,WACnBuG,EAAWlM,EAAK2F,YACd,IAAIsG,UAAWpJ,EAAQ,+DAAgE,YAAa7C,EAAK2F,YAG7GmI,EAAYR,EAAS,UACzBtN,EAAKuP,KAAOjC,EAAQiC,MACdrD,EAAWlM,EAAKuP,OACd,IAAItD,UAAWpJ,EAAQ,+DAAgE,OAAQ7C,EAAKuP,QAGxGzB,EAAYR,EAAS,UACzBtN,EAAKiJ,KAAOqE,EAAQrE,MAEd,KACR,CVsFQsF,CAAUvO,EAAMsN,IAErB,MAAME,EAGR,GAAKxN,EAAK0I,KAAM,GAAMT,EACrB,MAAM,IAAIrF,WAAYC,EAAQ,sHAAuH,OAAQ7C,EAAK0I,KAAM,KAgDzK,OA7CAuG,EAtID,SAAwBhH,EAAG5E,GAC1B,IAAIoC,EAAM,CAAA,EAGV,OAFAA,EAAIkD,UAAYjE,GAAcuD,EAAG5E,GAAO,GACxCoC,EAAIsB,MAAQrC,GAAcuD,EAAG4D,IAAQ,GAC9BpG,CACR,CAiIWoK,CAAe5H,EAAG5E,GAG5B0D,EAAQrC,GAAcuD,EAAG4D,IAAQ,GAGjCjD,EW5KD,SAAgBnD,EAAKwC,GACpB,IAAI6H,EACA5L,EAIJ,IADA4L,EAAK,GACC5L,EAAI,EAAGA,EAAI+D,EAAG/D,IACnB4L,EAAG1L,KAAM,GAGV,OAUA,SAAsBpB,EAAGyE,GACxB,IAAIsI,EACAC,EACAjQ,EAiBJ,OAdAA,EAAI0F,EAAIkG,IAAK3I,EAAG,GAAM,EACtByC,EAAIwK,IAAKjN,EAAG,EAAGjD,GAGf0F,EAAIwK,IAAKjN,EAAG,EAAGyC,EAAIkG,IAAK3I,EAAG,GAAIyE,GAG/BuI,EAAKvK,EAAIkG,IAAK3I,EAAG,GAEjBgN,IADAD,EAAQtI,EAAOuI,GACDjQ,EACd+P,EAAI9M,IAAO+M,GAAUtI,EAAKuI,GAC1BvK,EAAIwK,IAAKjN,EAAG,EAAGgN,GACfvK,EAAIwK,IAAKjN,EAAG,EAAG+C,EAAM+J,EAAG9M,IAAIjD,EAAE,KAEvB0F,CACP,CACF,CXkIgByK,CAAYnJ,EAAOkB,GAGb,WAAhBjI,EAAKgJ,QACTvB,EAAOH,GAGFtH,EAAKuP,OACTL,EAAQ3J,GAAclC,GAAO,KAEH,gBAAhBrD,EAAKgJ,QAChBvB,EAAOF,GAGFvH,EAAK2F,YACTkD,EY3LH,SAAoBxF,GACnB,IAAIzB,EAEA4H,EACA/D,EACAkE,EACAzF,EAcJ,IALAtC,EANS,GAGT6D,EAAM,IAAI5E,EAHD,EAGewC,IAGF8M,kBAGtB3G,EAAM,GACNG,EAAK,EACCzF,EAAI,EAAGA,EAAIb,EAAOa,IACvBsF,EAAIpF,KAAMgM,EAAe,IAAIvP,EAAc4E,EAAIqD,OAAQa,EAZ/C,KAaRA,GAAM/H,EAEP,OASA,SAAsByO,GACrB,IAAInM,EACJ,GAA0B,IAArByJ,UAAUzK,OACd,OAAOuC,EAER,IAAMvB,EAAI,EAAGA,EAAIb,EAAOa,IACvBsF,EAAKtF,GAAKmM,EAAI1E,IAAKzH,IAEpB,OAAOuB,CACP,CACF,CZgJe6K,CAAgBjN,IAGxBrD,EAAKuP,OACTL,EAAQ3J,GAAclC,GAAO,KAG9BoE,EAAOT,QAGW,IAAd2B,GAEJA,EAAYjE,GAAcuD,EAAG5E,GAAO,GAGpCqF,EAAO6H,GAAgB5H,EAAW5B,EAAO6B,EAAcC,EAAWpB,EAAMzH,IAGxEgF,GAAYiK,EAAQtG,UAAWA,GAGhC3H,EAAawN,EAAa,OAAQxO,EAAKiJ,MACvCjI,EAAawN,EAAa,WA+G1B,SAAkB/I,EAAKtC,GACtB,IAAIG,EACA+H,EACA3D,EACAzC,EACAC,EACAqG,EACAxH,EACA7C,EACAsP,EACAxN,EACAkB,EACJ,GAAKyJ,UAAUzK,OAAS,EAAI,CAC3B,IAAM4L,EAAcrJ,GACnB,MAAM,IAAIwG,UAAWpJ,EAAQ,oFAAqF4C,IAEnH+K,EAAI/K,EACJvE,EAAIiC,CACP,MACGjC,EAAIuE,EAEL,IAAM2J,EAAclO,GACnB,MAAM,IAAI+K,UAAWpJ,EAAQ,yEAA0E3B,IAExG,GAAKA,EAAEK,MAAO,KAAQ8B,EACrB,MAAM,IAAIsL,MAAO9L,EAAQ,2GAA4GQ,EAAOnC,EAAEK,MAAO,KAEtJ,QAAW,IAANiP,EACJA,EAAIjL,GAAcrE,EAAEK,MAAO,IAAK,QAC1B,GAAKiP,EAAEtN,SAAWhC,EAAEK,MAAO,GACjC,MAAM,IAAIoN,MAAO9L,EAAQ,6GAA8G3B,EAAEK,MAAO,GAAKiP,EAAEtN,SAExJ,GAAKwF,EACJ,OAAO,KAERhB,EAAOxG,EAAEK,MAAO,GAGXvB,EAAK2F,YACY,WAAhB3F,EAAKgJ,QACJhJ,EAAKuP,OACTrO,EAAI8D,GAAYN,GAAcgD,EAAMrE,GAAO,GAAQnC,IAEpDA,EAAIuP,GAAiBvP,IACM,gBAAhBlB,EAAKgJ,SACXhJ,EAAKuP,OACTrO,EAAI8D,GAAYN,GAAcgD,EAAMrE,GAAO,GAAQnC,IAEpDA,EAAIwP,GAAmBxP,EAAG2H,OAY5B,IATAwC,EAAO1C,EAAUnH,KACjB+J,EAAK5C,EAAUlH,QAAS,GAExB6B,EAAOpC,EAAEM,KACTyD,EAAM/D,EAAEO,QAAS,GACjByD,EAAMhE,EAAEO,QAAS,GACjBsC,EAAK7C,EAAEQ,OAGDwC,EAAI,EAAGA,EAAIwD,EAAMxD,IACtBlB,EAAIgF,GAAiBP,EAAMQ,EAAG5E,EAAOgI,EAAME,EAAI,EAAGjI,EAAM4B,EAAKnB,GAG7DyM,EAAEP,IAAK/L,EAAGlB,GAGVe,GAAMkB,EAEP,OAAOuL,CACP,IAnLMhC,EAWP,SAASA,EAAa6B,GACrB,IACIhF,EACAsF,EACAC,EACAC,EACAtF,EACAuF,EACArF,EACA5J,EACA9B,EACAuC,EACAU,EACJ,GAA0B,IAArB2K,UAAUzK,OACd,OAAKwF,EACG,KAEDuG,EAGR,IAAMH,EADNjN,EAAIwO,GAEH,MAAM,IAAIpE,UAAWpJ,EAAQ,yEAA0EhB,IAExG,GAAKA,EAAEN,MAAO,KAAQ8B,EACrB,MAAM,IAAIsL,MAAO9L,EAAQ,kGAAmGQ,EAAOxB,EAAEN,MAAO,KAO7I,GAJKsH,GACJA,EAAWhH,GAGP6G,EAAO,CAEX,IAAc,IADPA,EAAM7G,GAEZ,OAAO,KAGR6G,OAAO,CACV,MAEQ1I,EAAK2F,YACY,WAAhB3F,EAAKgJ,QACJhJ,EAAKuP,OACT1N,EAAI2D,GAAY0J,EAAOrN,IAExB8D,GAAWtC,EAAOxB,EAAEL,KAAMK,EAAEJ,QAAS,GAAKI,EAAEH,SACjB,gBAAhB1B,EAAKgJ,SACXhJ,EAAKuP,OACT1N,EAAI2D,GAAY0J,EAAOrN,IAExB+O,EAAO/H,IAGPvC,GAAajD,EAAOxB,EAAEL,KAAMK,EAAEJ,QAAS,GAAKI,EAAEH,OAAQkP,EAAM,EAAG,EAAGA,EAAM,EAAG,KAG7EvF,EAAO1C,EAAUnH,KACjB+J,EAAK5C,EAAUlH,QAAS,GAExBkP,EAAO9O,EAAEL,KACTqP,EAAKhP,EAAEJ,QAAS,GAChBqP,EAAKjP,EAAEH,OAMP+J,EAAKF,GAHLvI,EAAIgF,GAAiBP,EAAMQ,EAAG5E,EAAOgI,EAAME,EAAI,EAAGoF,EAAME,EAAIC,IAM5D/Q,EAAIgH,EAAM4E,IAAK3I,EAAG,GAAM,EACxByF,GAAgBpF,EAAOtD,EAAGsL,EAAM,EAAGI,EAAIkF,EAAME,EAAIC,GAGjDxO,EAAImF,EAAMpE,EAAOgI,EAAM,EAAGI,EAAIkF,EAAME,EAAIC,GAGxClI,EAAc5F,EAAGV,GAMlB,OAHAW,EAAO0F,EAAUzF,OAAQyF,EAAUnH,KAAM,EAAGyN,EAAQtG,UAAUnH,KAAM,GACpEyB,EAAO8D,EAAM7D,OAAQ6D,EAAMvF,KAAM,EAAGyN,EAAQlI,MAAMvF,KAAM,GAEjDyN,CACP,CAqFF,IOxWAjO,EAAAqM,GAAA,qBMcA,SAA4BC,GAC3B,IAAIyD,EACAC,EACApQ,EACAqQ,EACAzC,EACAxO,EACAwN,EAGJ,GADAxN,EAAOuP,EAAM2B,IACRvD,UAAUzK,OAAS,IACvBsK,ECpCF,SAAmBxN,EAAMsN,GACxB,OAAMO,EAAUP,GAGXQ,EAAYR,EAAS,aACzBtN,EAAK+M,QAAUO,EAAQP,SACjBqB,EAAkBpO,EAAK+M,UACrB,IAAId,UAAWpJ,EAAQ,uEAAwE,UAAW7C,EAAK+M,UAGnHe,EAAYR,EAAS,UACzBtN,EAAKmR,KAAO7D,EAAQ6D,MACd/C,EAAkBpO,EAAKmR,OACrB,IAAIlF,UAAWpJ,EAAQ,uEAAwE,OAAQ7C,EAAKmR,OAGhHrD,EAAYR,EAAS,YACzBtN,EAAKwC,OAAS8K,EAAQ9K,QAChBuL,EAAqB/N,EAAKwC,SACxB,IAAIyJ,UAAWpJ,EAAQ,0EAA2E,SAAU7C,EAAKwC,SAGrHsL,EAAYR,EAAS,kBACzBtN,EAAKQ,aAAe8M,EAAQ9M,cACtB4Q,GAAUpR,EAAKQ,eACb,IAAIyL,UAAWpJ,EAAQ,8DAA+D,eAAgB7C,EAAKQ,eAG/GsN,EAAYR,EAAS,UACzBtN,EAAKU,KAAO4M,EAAQ5M,MACd0Q,GAAUpR,EAAKU,OACb,IAAIuL,UAAWpJ,EAAQ,8DAA+D,OAAQ7C,EAAKU,OAGvGoN,EAAYR,EAAS,eACzBtN,EAAKW,UAAY2M,EAAQ3M,WACnBuL,EAAWlM,EAAKW,YACd,IAAIsL,UAAWpJ,EAAQ,+DAAgE,YAAa7C,EAAKW,YAG3G,KAtCC,IAAIsL,UAAWpJ,EAAQ,qEAAsEyK,GAuCtG,CDLQiB,CAAUvO,EAAMsN,IAErB,MAAME,EAMR,OAHA5M,EAAW,KAGFZ,EAAKU,MACd,IAAK,qBACJsQ,EAAWlE,GACZ,MACA,IAAK,QACJkE,EAAW5D,GACZ,MACA,IAAK,eACJ4D,EAAW7D,GACZ,MACA,QACC,MAAMwB,MAAO9L,EAAQ,gFAAiF,OAAQ,CAAE,qBAAsB,QAAS,gBAAiBsL,KAAM,QAAUnO,EAAKU,OAoBtL,SAAS8N,EAAatN,EAAGa,GACxB,IAAIC,EAEJ,IAAMqP,EAASnQ,GACd,MAAM,IAAI+K,UAAWpJ,EAAQ,kEAAmE3B,IAMjG,GAJMN,IACLA,EAAW,IAAIkL,GAAc5K,EAAEgC,OAAQlD,EAAKW,WAC5CoQ,EAAa/Q,EAAKW,UAAYC,EAASuL,SAAW,EAAIvL,EAASuL,UAE3DjL,EAAEgC,SAAW6N,EACjB,MAAM,IAAI9E,UAAWpJ,EAAQ,+EAAgFkO,EAAY7P,IAI1Hc,EAAMiP,IAGND,EAAUpQ,EAAUM,EAAGa,EAAGC,EAAKhC,EAAKwC,OAAQxC,EAAK+M,QACjD,CAID,OAvCAkE,EE7ED,SAAkB7N,EAAM+N,EAAM3O,GAC7B,IAAI8O,EACA9E,EAIJ,OAFA8E,EAAO,EAEElO,GACT,IAAK,QAEJoJ,EAmBD,WACC,IAAIxK,EAAM,KAAWsP,EAAO,KAE5B,OADAA,GAAQ,EACDtP,CACP,EAtBA,MACD,IAAK,WACJwK,EA4BD,WAEC,OADA8E,GAAQ,EACDH,CACP,EA9BA,MACD,IAAK,UACJ3E,EAoCD,WACC,IAAIxK,EAAM,GAAQQ,EAAS8O,GAE3B,OADAA,GAAQ,EACDtP,CACP,EAvCA,MACD,QACC,MAAM,IAAI2M,MAAO9L,EAAQ,gFAAiF,eAAgB,CAAE,QAAS,WAAY,WAAYsL,KAAM,QAAU/K,IAE9K,OAAOoJ,CAoCR,CFqBW+E,CAAQvR,EAAKQ,aAAcR,EAAKmR,KAAMnR,EAAKwC,QAqCrDgP,EAAkChD,EAAa,SAe/C,WACC,IAAIhC,EACAtI,EAGJ,IADAsI,EAAM,IAAIrB,MAAOvK,EAASuL,UACpBjI,EAAI,EAAGA,EAAIsI,EAAItJ,OAAQgB,IAC5BsI,EAAKtI,GAAMtD,EAASwL,MAAOlI,GAAMtD,EAASO,MAE3C,OAAOqL,CACP,IAvBDxL,EAAawN,EAAa,WAuC1B,SAAkBtN,GACjB,IAAMmQ,EAASnQ,IAAOA,EAAEgC,SAAW6N,EAClC,MAAM,IAAI9E,UAAWpJ,EAAQ,+EAAgFkO,GAAc,EAAG7P,IAE/H,OAAON,EAASqM,aAAc/L,EAC9B,IA3CMsN,CA4CR,IGjKA,IAAAnB,GAAA,CAAA,EAUArM,EAAAqM,GAAA,OAAAoE"}